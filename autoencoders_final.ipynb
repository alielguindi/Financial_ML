{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import zipfile as zp\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import os\n",
    "import sklearn as sk\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.23.5\n",
      "Pandas version:  1.5.1\n",
      "Tensorflow version:  2.12.0\n",
      " Sklearn version: 1.2.1\n",
      " Scipy version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "# Print the version of numpy sklearn and tensorflow\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(f\" Sklearn version: {sk.__version__}\")\n",
    "print(f\" Scipy version: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the zip file in context manager\n",
    "with zp.ZipFile('Data.zip', 'r') as myzip:\n",
    "    #List the files in the zip file\n",
    "    # myzip.printdir()\n",
    "    # Save printdir to a variable\n",
    "    files = myzip.namelist()\n",
    "files_sorted = sorted(files[3:])\n",
    "files_from_1970 = files_sorted[550:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intersection.pkl', 'rb') as f:\n",
    "    intersection = pickle.load(f)\n",
    "intersection.append('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_matrix(intersection, files_from_1970):\n",
    "    z = np.zeros((1, 70))\n",
    "    x = np.zeros((1, 70))\n",
    "    returns = np.array([])\n",
    "    number_of_obs = {}\n",
    "    for i, file in enumerate(files_from_1970):\n",
    "        full_name = 'Data/' + file\n",
    "        df = pd.read_csv(full_name)\n",
    "        df.drop(columns=['Unnamed: 0', 'mve0', 'prc', 'SHROUT', 'sic2'], inplace=True)\n",
    "        df = df[intersection]\n",
    "        df.dropna(axis=0, thresh=60, inplace=True)\n",
    "        # df.drop(columns=['RET'], inplace=True)\n",
    "        df = df.apply(lambda x: (x - x.mean()) / x.std() if (x.name not in ('permno','RET','DATE'))  else x)\n",
    "        df.fillna(0, inplace=True)\n",
    "        Z = df.drop(columns=['permno', 'RET', 'DATE'])\n",
    "        Z = Z.to_numpy()\n",
    "        X = np.linalg.inv(Z.T @ Z) @ Z.T @ df['RET'].to_numpy()\n",
    "        X_stacked = np.tile(X, (len(Z), 1))\n",
    "        ret = df['RET'].to_numpy()\n",
    "        number_of_obs[file] = (len(ret))\n",
    "        z = np.vstack((z, Z))\n",
    "        x = np.vstack((x, X_stacked))\n",
    "        returns = np.append(returns, ret)\n",
    "        print(f\"File {file} read successfully.\")\n",
    "    z = z[1:]\n",
    "    x = x[1:]\n",
    "    return z, x, returns, number_of_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_files = False\n",
    "if reload_files == True:\n",
    "    Z, X, returns, number_of_obs = get_data_to_matrix(intersection, files_from_1970)\n",
    "\n",
    "    with open('characters.pkl', 'wb') as f:\n",
    "        pickle.dump(Z, f)\n",
    "    with open('factors.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "    with open('returns.pkl', 'wb') as f:\n",
    "        pickle.dump(returns, f)\n",
    "    with open('number_of_obs.pkl', 'wb') as f:\n",
    "        pickle.dump(number_of_obs, f)\n",
    "else:\n",
    "    with open('characters.pkl', 'rb') as f:\n",
    "        Z = pickle.load(f)\n",
    "    with open('factors.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    with open('returns.pkl', 'rb') as f:\n",
    "        returns = pickle.load(f)\n",
    "    with open('number_of_obs.pkl', 'rb') as f:\n",
    "        number_of_obs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026950\n"
     ]
    }
   ],
   "source": [
    "# Sum the values in number_of_obs until we get to 2005 in the keys\n",
    "sum_train = 0\n",
    "for key, value in number_of_obs.items():\n",
    "    if '20050129' in key:\n",
    "        print(sum_train)\n",
    "        break\n",
    "    sum_train += value\n",
    "\n",
    "sum_val = 0\n",
    "for key, value in number_of_obs.items():\n",
    "    if '20100129' in key:\n",
    "        print(sum_val)\n",
    "        break\n",
    "    sum_val += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z_test = Z[2026950:]\n",
    "x_test = X[2026950:]\n",
    "returns_test = returns[2026950:]\n",
    "z_x_test = np.hstack((z_test, x_test))\n",
    "\n",
    "z_x_train = np.concatenate((Z[:1737557], X[:1737557]), axis=1)\n",
    "z_x_val = np.concatenate((Z[1737557:2026950], X[1737557:2026950]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop= EarlyStopping(monitor='val_loss', patience=3)\n",
    "def create_one_hidden_layer_mod(no_units_activ, no_factors, activ, l_rate, inp_shape, reg, reg_type, kernel_init, optimizer):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    # Split the two inputs into two different layers\n",
    "    print(inputs_2)\n",
    "    x_1 = Dense(units=no_units_activ,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(inputs_1)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=reg_type(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            # input_shape=(inp_shape,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=reg_type(reg)\n",
    "            )(x_1)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=optimizer(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_hidden_layer_network(no_units_first, no_units_second, no_factors, activ, l_rate,  inp_shape, reg, reg_type, kernel_init):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "    # print(inputs)\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "    # print(inputs_1)\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    x_1 = Dense(units=no_units_first,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(inputs_1)\n",
    "    x_1_hidden = Dense(units=no_units_second,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(x_1)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=reg_type(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=reg_type(reg)\n",
    "            # kernel_regularizer=keras.regularizers.L1(0.001),\n",
    "            )(x_1_hidden)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_three_hidden_layer_network(no_units_first, no_units_second, no_units_third, no_factors, activ, l_rate, inp_shape, reg, reg_type, kernel_init):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "    # print(inputs)\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "    # print(inputs_1)\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    x_1 = Dense(units=no_units_first,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(inputs_1)\n",
    "    x_1_hidden = Dense(units=no_units_second,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(x_1)\n",
    "    x_1_hidden_2 = Dense(units=no_units_third,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=reg_type(reg))(x_1_hidden)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=reg_type(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=keras.regularizers.L1(reg)\n",
    "            # kernel_regularizer=keras.regularizers.L1(0.001),\n",
    "            )(x_1_hidden_2)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net_distributions_one_hh  = {\n",
    "    'no_units_activ': [16, 32, 64],\n",
    "    'activ': ['relu', 'sigmoid'],\n",
    "    'reg_type': [keras.regularizers.l1, keras.regularizers.l2],\n",
    "    'reg': [0, 0.001, 0.01],\n",
    "    'l_rate': [0.0001, 0.001, 0.01],\n",
    "    'kernel_init': ['he_uniform'],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10, 20],\n",
    "    'inp_shape': [z_x_train.shape[1]],\n",
    "    'no_factors': [3]}\n",
    "\n",
    "neural_net_distributions_two_hh  = {\n",
    "    'no_units_first': [16, 32, 64],\n",
    "    'no_units_second': [16, 32, 64],\n",
    "    'activ': ['relu', 'sigmoid'],\n",
    "    'reg_type': [keras.regularizers.l1, keras.regularizers.l2],\n",
    "    'reg': [0, 0.001, 0.01],\n",
    "    'l_rate': [0.0001, 0.001, 0.01],\n",
    "    'kernel_init': ['he_uniform'],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10, 20, 50],\n",
    "    'inp_shape': [z_x_train.shape[1]],\n",
    "    'no_factors': [3]}\n",
    "\n",
    "neural_net_distributions_three_hh  = {\n",
    "    'no_units_first': [16, 32, 64],\n",
    "    'no_units_second': [16, 32, 64],\n",
    "    'no_units_third': [16, 32, 64],\n",
    "    'activ': ['relu', 'sigmoid'],\n",
    "    'reg_type': [keras.regularizers.l1, keras.regularizers.l2],\n",
    "    'reg': [0, 0.001, 0.01],\n",
    "    'l_rate': [0.0001, 0.001, 0.01],\n",
    "    'kernel_init': ['he_uniform'],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10, 20, 50],\n",
    "    'inp_shape': [z_x_train.shape[1]],\n",
    "    'no_factors': [3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop= EarlyStopping(monitor='val_loss', patience=3)\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# import randint, unfirom from scipy.stats\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = Z[:1737557]\n",
    "x_train = X[:1737557]\n",
    "returns_train = returns[:1737557]\n",
    "# x_train = np.hstack((z_train, x_train))\n",
    "\n",
    "z_val = Z[1737557:2026950]\n",
    "x_val = X[1737557:2026950]\n",
    "returns_val = returns[1737557:2026950]\n",
    "# x_val = np.hstack((z_val, x_val))\n",
    "\n",
    "# z_test = Z[2026950:]\n",
    "# x_test = X[2026950:]\n",
    "# returns_test = returns[2026950:]\n",
    "# x_test = np.hstack((z_test, x_test))\n",
    "\n",
    "z_x_train = np.concatenate((Z[:1737557], X[:1737557]), axis=1)\n",
    "z_x_val = np.concatenate((Z[1737557:2026950], X[1737557:2026950]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop= EarlyStopping(monitor='val_loss', patience=3)\n",
    "def create_one_hidden_layer_mod(no_units_activ,no_factors, activ, l_rate, inp_shape, reg):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "    # print(inputs)\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "    # print(inputs_1)\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    print(inputs_2)\n",
    "    x_1 = Dense(units=no_units_activ,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(inputs_1)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            # input_shape=(inp_shape,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg)\n",
    "            )(x_1)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 140), dtype=tf.float32, name='Input'), name='Input', description=\"created by layer 'Input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_4/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_5/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_5'\")\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 17ms/step - loss: 0.0319 - val_loss: 0.0293\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 2s 13ms/step - loss: 0.0303 - val_loss: 0.0287\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 2s 13ms/step - loss: 0.0299 - val_loss: 0.0287\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 2s 12ms/step - loss: 0.0298 - val_loss: 0.0286\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 2s 12ms/step - loss: 0.0297 - val_loss: 0.0286\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 2s 12ms/step - loss: 0.0296 - val_loss: 0.0287\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 2s 12ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 2s 12ms/step - loss: 0.0295 - val_loss: 0.0287\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(z_x_train, returns_train, epochs=20, batch_size=10000, validation_data=(z_x_val, returns_val), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berta\\AppData\\Local\\Temp\\ipykernel_34624\\1500825809.py:6: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_one_hidden_layer_mod, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_1/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_1'\")\n",
      "Epoch 1/50\n",
      "1738/1738 [==============================] - 4s 2ms/step - loss: 0.0305 - val_loss: 0.0289\n",
      "Epoch 2/50\n",
      "1738/1738 [==============================] - 2s 1ms/step - loss: 0.0298 - val_loss: 0.0286\n",
      "Epoch 3/50\n",
      "1738/1738 [==============================] - 2s 1ms/step - loss: 0.0297 - val_loss: 0.0285\n",
      "Epoch 4/50\n",
      "1738/1738 [==============================] - 2s 1ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "Epoch 5/50\n",
      "1738/1738 [==============================] - 2s 1ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "Epoch 6/50\n",
      "1738/1738 [==============================] - 3s 1ms/step - loss: 0.0296 - val_loss: 0.0286\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to find the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop= EarlyStopping(monitor='val_loss', patience=3)\n",
    "model = KerasRegressor(build_fn=create_one_hidden_layer_mod, verbose=1)\n",
    "# define the grid search parameters\n",
    "no_units_activs = [16, 32, 64]\n",
    "# no_factors = [3, 5, 7]\n",
    "activs = ['relu']\n",
    "# regularizers = [0, 0.001, 0.01]\n",
    "# learning_rates = [0.0002, 0.002, 0.001]\n",
    "batch_sizes = [50, 100, 500, 1000]\n",
    "epochs = [20, 50, 100]\n",
    "# earlystops = [EarlyStopping(monitor='val_loss', patience=3),EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "param_grid = dict(no_units_activ=no_units_activs, no_factors=[3], activ=activs,\n",
    "                  inp_shape=[z_x_train.shape[1]], reg=[0], l_rate=[0.002], batch_size=batch_sizes, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=3, cv=3)\n",
    "grid_result = grid.fit(z_x_train, returns[:1737557], validation_data=(z_x_val, returns[1737557:2026950]), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activ': 'relu',\n",
       " 'batch_size': 1000,\n",
       " 'epochs': 50,\n",
       " 'inp_shape': 140,\n",
       " 'l_rate': 0.002,\n",
       " 'no_factors': 3,\n",
       " 'no_units_activ': 16,\n",
       " 'reg': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 140), dtype=tf.float32, name='Input'), name='Input', description=\"created by layer 'Input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_2/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_3/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_3'\")\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1738/1738 [==============================] - 8s 4ms/step - loss: 0.0304 - val_loss: 0.0288\n",
      "Epoch 2/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0298 - val_loss: 0.0286\n",
      "Epoch 3/20\n",
      "1738/1738 [==============================] - 6s 4ms/step - loss: 0.0296 - val_loss: 0.0285\n",
      "Epoch 4/20\n",
      "1738/1738 [==============================] - 6s 4ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "Epoch 5/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0295 - val_loss: 0.0284\n",
      "Epoch 6/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0295 - val_loss: 0.0286\n",
      "Epoch 7/20\n",
      "1738/1738 [==============================] - 6s 4ms/step - loss: 0.0294 - val_loss: 0.0285\n",
      "Epoch 8/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0294 - val_loss: 0.0285\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    x=z_x_train,\n",
    "    y=returns[:1737557],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_val, returns[1737557:2026950]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9044/9044 [==============================] - 15s 2ms/step\n",
      "Out of Sample R^2: 0.12401829972240086\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_val)\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"Out of Sample R^2: {r2_score(returns[1737557:2026950], pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_hidden_layer_network(no_units_first, no_units_second,no_factors, activ, l_rate, inp_shape, reg):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "    # print(inputs)\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "    # print(inputs_1)\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    x_1 = Dense(units=no_units_first,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(inputs_1)\n",
    "    x_1_hidden = Dense(units=no_units_second,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(x_1)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg)\n",
    "            # kernel_regularizer=keras.regularizers.L1(0.001),\n",
    "            )(x_1_hidden)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=32, no_units_second=16, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1738/1738 [==============================] - 10s 5ms/step - loss: 0.0303 - val_loss: 0.0288\n",
      "Epoch 2/20\n",
      "1738/1738 [==============================] - 8s 5ms/step - loss: 0.0297 - val_loss: 0.0288\n",
      "Epoch 3/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "Epoch 4/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0295 - val_loss: 0.0286\n",
      "Epoch 5/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0295 - val_loss: 0.0287\n",
      "Epoch 6/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0294 - val_loss: 0.0285\n",
      "Epoch 7/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0293 - val_loss: 0.0291\n",
      "Epoch 8/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 9/20\n",
      "1738/1738 [==============================] - 7s 4ms/step - loss: 0.0293 - val_loss: 0.0287\n"
     ]
    }
   ],
   "source": [
    "history = model_2.fit(z_x_train, returns[:1737557], epochs=20, batch_size=1000, validation_data=(z_x_val, returns[1737557:2026950]), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berta\\AppData\\Local\\Temp\\ipykernel_1732\\99865868.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_2 = KerasRegressor(build_fn=create_two_hidden_layer_network)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738/1738 [==============================] - 13s 6ms/step - loss: 0.0303 - val_loss: 0.0288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model_2 = KerasRegressor(build_fn=create_two_hidden_layer_network)\n",
    "# define the grid search parameters\n",
    "no_first = [8, 16, 32, 64, 128]\n",
    "no_second = [8, 16, 32, 64, 128]\n",
    "# no_factors = [3, 5, 7]\n",
    "# activs = ['relu', 'gelu']\n",
    "activs = ['relu']\n",
    "# regularizers = [0, 0.001, 0.01]\n",
    "# learning_rates = [0.0002, 0.002, 0.001]\n",
    "# batch_sizes = [1000, 5000,10000, 20000]\n",
    "epochs = [50]\n",
    "\n",
    "param_grid_2 = dict(no_units_first=no_first, no_units_second=no_second,\n",
    "                     no_factors=[3],activ=activs, inp_shape=[z_x_train.shape[1]],\n",
    "                       reg=[0], l_rate=[0.002], batch_size=[1000])\n",
    "grid_2 = GridSearchCV(estimator=model_2, param_grid=param_grid_2,n_jobs=3, cv=3)\n",
    "grid_result_2 = grid_2.fit(z_x_train, returns[:1737557], validation_data=(z_x_val, returns[1737557:2026950]), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activ': 'relu',\n",
       " 'batch_size': 1000,\n",
       " 'inp_shape': 140,\n",
       " 'l_rate': 0.002,\n",
       " 'no_factors': 3,\n",
       " 'no_units_first': 64,\n",
       " 'no_units_second': 32,\n",
       " 'reg': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_result_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_three_hidden_layer_network(no_units_first, no_units_second, no_units_third, no_factors, activ, l_rate, inp_shape, reg):\n",
    "    inputs = keras.Input(shape=(inp_shape,), name='Input')\n",
    "    # print(inputs)\n",
    "    layer_input_dim = inp_shape // 2\n",
    "    inputs_1 = inputs[:,:70]\n",
    "    # print(inputs_1)\n",
    "    inputs_2 = inputs[:,70:]\n",
    "    x_1 = Dense(units=no_units_first,\n",
    "        input_shape=(layer_input_dim,),\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(inputs_1)\n",
    "    x_1_hidden = Dense(units=no_units_second,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(x_1)\n",
    "    x_1_hidden_2 = Dense(units=no_units_third,\n",
    "        activation=activ,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='he_uniform',\n",
    "        kernel_regularizer=keras.regularizers.L1(reg))(x_1_hidden)\n",
    "    x_2_out = Dense(units=no_factors,\n",
    "            input_shape=(layer_input_dim,),\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg))(inputs_2)\n",
    "    x_1_out = Dense(units=no_factors,\n",
    "            activation='linear',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.L1(reg)\n",
    "            # kernel_regularizer=keras.regularizers.L1(0.001),\n",
    "            )(x_1_hidden_2)\n",
    "    #The output will be the dot product of the two hidden layers\n",
    "    outputs = keras.layers.Dot(axes=1)([x_1_out, x_2_out])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='autoencoder')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=l_rate),\n",
    "        loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we Restrict the Model parameters although before this final version we have tried grid-search for differen batch-sizes, learning rates and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berta\\AppData\\Local\\Temp\\ipykernel_1732\\2567158883.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_3 = KerasRegressor(build_fn=create_three_hidden_layer_network)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738/1738 [==============================] - 14s 7ms/step - loss: 0.0304 - val_loss: 0.0283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_3 = KerasRegressor(build_fn=create_three_hidden_layer_network)\n",
    "# define the grid search parameters\n",
    "no_first = [8, 16, 32, 64, 128]\n",
    "no_second = [8, 16, 32, 64, 128]\n",
    "no_third = [8, 16, 32, 64, 128]\n",
    "# no_factors = [3, 5, 7]\n",
    "# activs = ['relu', 'gelu']\n",
    "# regularizers = [0, 0.001, 0.01]\n",
    "# learning_rates = [0.0002, 0.002, 0.001]\n",
    "# batch_sizes = [1000, 5000,10000, 20000]\n",
    "\n",
    "param_grid_3 = dict(no_units_first=no_first, no_units_second=no_second,\n",
    "                    no_units_third=no_third, no_factors=[3],activ=['relu'],\n",
    "                    l_rate=[0.002], inp_shape=[z_x_train.shape[1]],\n",
    "                    reg=[0], batch_size=[1000])\n",
    "grid_3 = GridSearchCV(estimator=model_3, param_grid=param_grid_3,n_jobs=3, cv=3)\n",
    "grid_result_3 = grid_3.fit(z_x_train, returns[:1737557], validation_data=(z_x_val, returns[1737557:2026950]), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activ': 'relu',\n",
       " 'batch_size': 1000,\n",
       " 'inp_shape': 140,\n",
       " 'l_rate': 0.002,\n",
       " 'no_factors': 3,\n",
       " 'no_units_first': 64,\n",
       " 'no_units_second': 128,\n",
       " 'no_units_third': 32,\n",
       " 'reg': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_result_3.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model at optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_x_train = np.concatenate((Z[:1737557], X[:1737557]), axis=1)\n",
    "z_x_val = np.concatenate((Z[1737557:2026950], X[1737557:2026950]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_r2(actual_returns, pred_returns):\n",
    "    denom = np.sum(actual_returns**2)\n",
    "    nom = np.sum((actual_returns - pred_returns)**2)\n",
    "    r_2 = 1 - nom / denom\n",
    "    return r_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_x_test = np.concatenate((Z[2026950:], X[2026950:]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_3/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_3'\")\n",
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0247\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 3s 1ms/step - loss: 0.0296 - val_loss: 0.0247\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 3s 1ms/step - loss: 0.0294 - val_loss: 0.0247\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 2s 1ms/step - loss: 0.0294 - val_loss: 0.0246\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 2s 1ms/step - loss: 0.0293 - val_loss: 0.0247\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 2s 1ms/step - loss: 0.0293 - val_loss: 0.0247\n",
      "Epoch 7/50\n",
      "2027/2027 [==============================] - 2s 1ms/step - loss: 0.0293 - val_loss: 0.0247\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out of Sample + Total $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample R^2: 0.11180776707959461\n",
      "Total R^2: 0.14398162478295573\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 9s 515us/step\n",
      "63343/63343 [==============================] - 34s 536us/step\n",
      "Out of Sample R^2: 0.09988282788810265\n",
      "Total R^2: 0.1401665390353658\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"Out of Sample R^2: {r2_score(returns[2026950:], pred)}\")\n",
    "print(f\"Total R^2: {r2_score(returns, np.concatenate([in_samp, pred]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive $R^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to get the weights corresponding to the factors to get the in Sample average and then use the weights corresponding to the betas to get the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This confirms we get the same output\n",
    "lambda_facts = np.mean(z_x_train[:,70:] @ model.get_weight_paths()['dense_4.kernel'].numpy() + np.tile(model.get_weight_paths()['dense_4.bias'].numpy(),(z_x_train[:,70:].shape[0],1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to confirm that we get the right output\n",
    "weights_first = model.get_weight_paths()['dense_3.kernel'].numpy()\n",
    "bias_first = model.get_weight_paths()['dense_3.bias'].numpy()\n",
    "weights_second = model.get_weight_paths()['dense_5.kernel'].numpy()\n",
    "bias_second = model.get_weight_paths()['dense_5.bias'].numpy()\n",
    "np.maximum(z_x_test[:,:70] @ weights_first + bias_first,0) @ weights_second + bias_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive R^{2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001571634861741722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0300 - val_loss: 0.0248\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 5s 2ms/step - loss: 0.0294 - val_loss: 0.0247\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 5s 3ms/step - loss: 0.0293 - val_loss: 0.0247\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0292 - val_loss: 0.0247\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 5s 2ms/step - loss: 0.0291 - val_loss: 0.0247\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 4s 2ms/step - loss: 0.0290 - val_loss: 0.0247\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 11s 641us/step\n",
      "63343/63343 [==============================] - 36s 567us/step\n",
      "Out of Sample R^2: 0.10936992490748976\n",
      "Total R^2: 0.15454757183175816\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive R^{2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038198255401545866"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 10s 552us/step\n",
      "63343/63343 [==============================] - 36s 571us/step\n",
      "Out of Sample R^2: 0.09550881572914593\n",
      "Total R^2: 0.17655499946706432\n"
     ]
    }
   ],
   "source": [
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {r2_score(returns[2026950:], pred)}\")\n",
    "print(f\"Total R^2: {r2_score(returns, np.concatenate([in_samp, pred]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 8s 3ms/step - loss: 0.0300 - val_loss: 0.0248\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0295 - val_loss: 0.0247\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0293 - val_loss: 0.0249\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0292 - val_loss: 0.0247\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0291 - val_loss: 0.0248\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0290 - val_loss: 0.0248\n",
      "Epoch 7/50\n",
      "2027/2027 [==============================] - 6s 3ms/step - loss: 0.0289 - val_loss: 0.0248\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 10s 578us/step\n",
      "63343/63343 [==============================] - 36s 558us/step\n",
      "Out of Sample R^2: 0.10232412097092813\n",
      "Total R^2: 0.15815373634395635\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 10s 560us/step\n",
      "63343/63343 [==============================] - 36s 571us/step\n",
      "Out of Sample R^2: 0.06729522696342505\n",
      "Total R^2: 0.2038230349076794\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "print(f\"Out of Sample R^2: {r2_score(returns[2026950:], pred)}\")\n",
    "print(f\"Total R^2: {r2_score(returns, np.concatenate([in_samp, pred]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive R^{2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003683647041954896"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_1/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_1'\")\n",
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 13s 5ms/step - loss: 0.0299 - val_loss: 0.0247\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 9s 4ms/step - loss: 0.0292 - val_loss: 0.0244\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0291 - val_loss: 0.0245\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0290 - val_loss: 0.0244\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0290 - val_loss: 0.0244\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 7s 4ms/step - loss: 0.0289 - val_loss: 0.0244\n",
      "Epoch 7/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0289 - val_loss: 0.0244\n",
      "Epoch 8/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0289 - val_loss: 0.0244\n",
      "Epoch 9/50\n",
      "2027/2027 [==============================] - 8s 4ms/step - loss: 0.0289 - val_loss: 0.0245\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=5, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample R^2: 0.11836962257042472\n",
      "Total R^2: 0.1542018069355764\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0012065008257415855"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 16s 7ms/step - loss: 0.0298 - val_loss: 0.0246\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 13s 6ms/step - loss: 0.0291 - val_loss: 0.0245\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 13s 6ms/step - loss: 0.0289 - val_loss: 0.0245\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 13s 6ms/step - loss: 0.0288 - val_loss: 0.0246\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 13s 6ms/step - loss: 0.0287 - val_loss: 0.0245\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 13s 6ms/step - loss: 0.0287 - val_loss: 0.0246\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=5, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out of Sample + Total $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 31s 2ms/step\n",
      "63343/63343 [==============================] - 117s 2ms/step\n",
      "Out of Sample R^2: 0.11562461496092236\n",
      "Total R^2: 0.1640287738784787\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive R^{2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004865792703996008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2027/2027 [==============================] - 18s 8ms/step - loss: 0.0298 - val_loss: 0.0248\n",
      "Epoch 2/50\n",
      "2027/2027 [==============================] - 15s 7ms/step - loss: 0.0291 - val_loss: 0.0245\n",
      "Epoch 3/50\n",
      "2027/2027 [==============================] - 15s 8ms/step - loss: 0.0289 - val_loss: 0.0245\n",
      "Epoch 4/50\n",
      "2027/2027 [==============================] - 15s 7ms/step - loss: 0.0288 - val_loss: 0.0244\n",
      "Epoch 5/50\n",
      "2027/2027 [==============================] - 15s 8ms/step - loss: 0.0287 - val_loss: 0.0245\n",
      "Epoch 6/50\n",
      "2027/2027 [==============================] - 15s 7ms/step - loss: 0.0286 - val_loss: 0.0246\n",
      "Epoch 7/50\n",
      "2027/2027 [==============================] - 15s 7ms/step - loss: 0.0285 - val_loss: 0.0246\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=5, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:2026950],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns[2026950:]),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out of Sample + Total $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17541/17541 [==============================] - 32s 2ms/step\n",
      "63343/63343 [==============================] - 112s 2ms/step\n",
      "Out of Sample R^2: 0.1141766769031084\n",
      "Total R^2: 0.16257605832847666\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns[2026950:], np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictive $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0046259527936586275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "create_total_r2(returns[2026950:], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for the Case when we restrict only for PCA Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_r2(actual_returns, pred_returns):\n",
    "    denom = np.sum(actual_returns**2)\n",
    "    nom = np.sum((actual_returns - pred_returns)**2)\n",
    "    r_2 = 1 - nom / denom\n",
    "    return r_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_columns.pkl', 'rb') as f:\n",
    "    pca_cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_matrix_pca_names_only(intersection, pca_names, files_from_1970):\n",
    "    z = np.zeros((1, 70))\n",
    "    x = np.zeros((1, 70))\n",
    "    returns = np.array([])\n",
    "    number_of_obs = {}\n",
    "    for i, file in enumerate(files_from_1970):\n",
    "        full_name = 'Data/' + file\n",
    "        df = pd.read_csv(full_name)\n",
    "        df.drop(columns=['Unnamed: 0', 'mve0', 'prc', 'SHROUT', 'sic2'], inplace=True)\n",
    "        df = df[intersection]\n",
    "        # Onl keep the pca columns\n",
    "        df = df[df.permno.isin(pca_names)]\n",
    "        df.dropna(axis=0, thresh=60, inplace=True)\n",
    "        # df.drop(columns=['RET'], inplace=True)\n",
    "        df = df.apply(lambda x: (x - x.mean()) / x.std() if (x.name not in ('permno','RET','DATE'))  else x)\n",
    "        df.fillna(0, inplace=True)\n",
    "        Z = df.drop(columns=['permno', 'RET', 'DATE'])\n",
    "        Z = Z.to_numpy()\n",
    "        X = np.linalg.inv(Z.T @ Z) @ Z.T @ df['RET'].to_numpy()\n",
    "        X_stacked = np.tile(X, (len(Z), 1))\n",
    "        ret = df['RET'].to_numpy()\n",
    "        number_of_obs[file] = (len(ret))\n",
    "        z = np.vstack((z, Z))\n",
    "        x = np.vstack((x, X_stacked))\n",
    "        returns = np.append(returns, ret)\n",
    "        print(f\"File {file} read successfully.\")\n",
    "    z = z[1:]\n",
    "    x = x[1:]\n",
    "    return z, x, returns, number_of_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_r2(actual_returns, pred_returns):\n",
    "    denom = np.sum(actual_returns**2)\n",
    "    nom = np.sum((actual_returns - pred_returns)**2)\n",
    "    r_2 = 1 - nom / denom\n",
    "    return r_2\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File monthly_data/data_for_month_19691231.csv read successfully.\n",
      "File monthly_data/data_for_month_19700130.csv read successfully.\n",
      "File monthly_data/data_for_month_19700227.csv read successfully.\n",
      "File monthly_data/data_for_month_19700331.csv read successfully.\n",
      "File monthly_data/data_for_month_19700430.csv read successfully.\n",
      "File monthly_data/data_for_month_19700529.csv read successfully.\n",
      "File monthly_data/data_for_month_19700630.csv read successfully.\n",
      "File monthly_data/data_for_month_19700731.csv read successfully.\n",
      "File monthly_data/data_for_month_19700831.csv read successfully.\n",
      "File monthly_data/data_for_month_19700930.csv read successfully.\n",
      "File monthly_data/data_for_month_19701030.csv read successfully.\n",
      "File monthly_data/data_for_month_19701130.csv read successfully.\n",
      "File monthly_data/data_for_month_19701231.csv read successfully.\n",
      "File monthly_data/data_for_month_19710129.csv read successfully.\n",
      "File monthly_data/data_for_month_19710226.csv read successfully.\n",
      "File monthly_data/data_for_month_19710331.csv read successfully.\n",
      "File monthly_data/data_for_month_19710430.csv read successfully.\n",
      "File monthly_data/data_for_month_19710528.csv read successfully.\n",
      "File monthly_data/data_for_month_19710630.csv read successfully.\n",
      "File monthly_data/data_for_month_19710730.csv read successfully.\n",
      "File monthly_data/data_for_month_19710831.csv read successfully.\n",
      "File monthly_data/data_for_month_19710930.csv read successfully.\n",
      "File monthly_data/data_for_month_19711029.csv read successfully.\n",
      "File monthly_data/data_for_month_19711130.csv read successfully.\n",
      "File monthly_data/data_for_month_19711231.csv read successfully.\n",
      "File monthly_data/data_for_month_19720131.csv read successfully.\n",
      "File monthly_data/data_for_month_19720229.csv read successfully.\n",
      "File monthly_data/data_for_month_19720330.csv read successfully.\n",
      "File monthly_data/data_for_month_19720428.csv read successfully.\n",
      "File monthly_data/data_for_month_19720531.csv read successfully.\n",
      "File monthly_data/data_for_month_19720630.csv read successfully.\n",
      "File monthly_data/data_for_month_19720731.csv read successfully.\n",
      "File monthly_data/data_for_month_19720831.csv read successfully.\n",
      "File monthly_data/data_for_month_19720929.csv read successfully.\n",
      "File monthly_data/data_for_month_19721031.csv read successfully.\n",
      "File monthly_data/data_for_month_19721130.csv read successfully.\n",
      "File monthly_data/data_for_month_19721229.csv read successfully.\n",
      "File monthly_data/data_for_month_19730131.csv read successfully.\n",
      "File monthly_data/data_for_month_19730228.csv read successfully.\n",
      "File monthly_data/data_for_month_19730330.csv read successfully.\n",
      "File monthly_data/data_for_month_19730430.csv read successfully.\n",
      "File monthly_data/data_for_month_19730531.csv read successfully.\n",
      "File monthly_data/data_for_month_19730629.csv read successfully.\n",
      "File monthly_data/data_for_month_19730731.csv read successfully.\n",
      "File monthly_data/data_for_month_19730831.csv read successfully.\n",
      "File monthly_data/data_for_month_19730928.csv read successfully.\n",
      "File monthly_data/data_for_month_19731031.csv read successfully.\n",
      "File monthly_data/data_for_month_19731130.csv read successfully.\n",
      "File monthly_data/data_for_month_19731231.csv read successfully.\n",
      "File monthly_data/data_for_month_19740131.csv read successfully.\n",
      "File monthly_data/data_for_month_19740228.csv read successfully.\n",
      "File monthly_data/data_for_month_19740329.csv read successfully.\n",
      "File monthly_data/data_for_month_19740430.csv read successfully.\n",
      "File monthly_data/data_for_month_19740531.csv read successfully.\n",
      "File monthly_data/data_for_month_19740628.csv read successfully.\n",
      "File monthly_data/data_for_month_19740731.csv read successfully.\n",
      "File monthly_data/data_for_month_19740830.csv read successfully.\n",
      "File monthly_data/data_for_month_19740930.csv read successfully.\n",
      "File monthly_data/data_for_month_19741031.csv read successfully.\n",
      "File monthly_data/data_for_month_19741129.csv read successfully.\n",
      "File monthly_data/data_for_month_19741231.csv read successfully.\n",
      "File monthly_data/data_for_month_19750131.csv read successfully.\n",
      "File monthly_data/data_for_month_19750228.csv read successfully.\n",
      "File monthly_data/data_for_month_19750331.csv read successfully.\n",
      "File monthly_data/data_for_month_19750430.csv read successfully.\n",
      "File monthly_data/data_for_month_19750530.csv read successfully.\n",
      "File monthly_data/data_for_month_19750630.csv read successfully.\n",
      "File monthly_data/data_for_month_19750731.csv read successfully.\n",
      "File monthly_data/data_for_month_19750829.csv read successfully.\n",
      "File monthly_data/data_for_month_19750930.csv read successfully.\n",
      "File monthly_data/data_for_month_19751031.csv read successfully.\n",
      "File monthly_data/data_for_month_19751128.csv read successfully.\n",
      "File monthly_data/data_for_month_19751231.csv read successfully.\n",
      "File monthly_data/data_for_month_19760130.csv read successfully.\n",
      "File monthly_data/data_for_month_19760227.csv read successfully.\n",
      "File monthly_data/data_for_month_19760331.csv read successfully.\n",
      "File monthly_data/data_for_month_19760430.csv read successfully.\n",
      "File monthly_data/data_for_month_19760528.csv read successfully.\n",
      "File monthly_data/data_for_month_19760630.csv read successfully.\n",
      "File monthly_data/data_for_month_19760730.csv read successfully.\n",
      "File monthly_data/data_for_month_19760831.csv read successfully.\n",
      "File monthly_data/data_for_month_19760930.csv read successfully.\n",
      "File monthly_data/data_for_month_19761029.csv read successfully.\n",
      "File monthly_data/data_for_month_19761130.csv read successfully.\n",
      "File monthly_data/data_for_month_19761231.csv read successfully.\n",
      "File monthly_data/data_for_month_19770131.csv read successfully.\n",
      "File monthly_data/data_for_month_19770228.csv read successfully.\n",
      "File monthly_data/data_for_month_19770331.csv read successfully.\n",
      "File monthly_data/data_for_month_19770429.csv read successfully.\n",
      "File monthly_data/data_for_month_19770531.csv read successfully.\n",
      "File monthly_data/data_for_month_19770630.csv read successfully.\n",
      "File monthly_data/data_for_month_19770729.csv read successfully.\n",
      "File monthly_data/data_for_month_19770831.csv read successfully.\n",
      "File monthly_data/data_for_month_19770930.csv read successfully.\n",
      "File monthly_data/data_for_month_19771031.csv read successfully.\n",
      "File monthly_data/data_for_month_19771130.csv read successfully.\n",
      "File monthly_data/data_for_month_19771230.csv read successfully.\n",
      "File monthly_data/data_for_month_19780131.csv read successfully.\n",
      "File monthly_data/data_for_month_19780228.csv read successfully.\n",
      "File monthly_data/data_for_month_19780331.csv read successfully.\n",
      "File monthly_data/data_for_month_19780428.csv read successfully.\n",
      "File monthly_data/data_for_month_19780531.csv read successfully.\n",
      "File monthly_data/data_for_month_19780630.csv read successfully.\n",
      "File monthly_data/data_for_month_19780731.csv read successfully.\n",
      "File monthly_data/data_for_month_19780831.csv read successfully.\n",
      "File monthly_data/data_for_month_19780929.csv read successfully.\n",
      "File monthly_data/data_for_month_19781031.csv read successfully.\n",
      "File monthly_data/data_for_month_19781130.csv read successfully.\n",
      "File monthly_data/data_for_month_19781229.csv read successfully.\n",
      "File monthly_data/data_for_month_19790131.csv read successfully.\n",
      "File monthly_data/data_for_month_19790228.csv read successfully.\n",
      "File monthly_data/data_for_month_19790330.csv read successfully.\n",
      "File monthly_data/data_for_month_19790430.csv read successfully.\n",
      "File monthly_data/data_for_month_19790531.csv read successfully.\n",
      "File monthly_data/data_for_month_19790629.csv read successfully.\n",
      "File monthly_data/data_for_month_19790731.csv read successfully.\n",
      "File monthly_data/data_for_month_19790831.csv read successfully.\n",
      "File monthly_data/data_for_month_19790928.csv read successfully.\n",
      "File monthly_data/data_for_month_19791031.csv read successfully.\n",
      "File monthly_data/data_for_month_19791130.csv read successfully.\n",
      "File monthly_data/data_for_month_19791231.csv read successfully.\n",
      "File monthly_data/data_for_month_19800131.csv read successfully.\n",
      "File monthly_data/data_for_month_19800229.csv read successfully.\n",
      "File monthly_data/data_for_month_19800331.csv read successfully.\n",
      "File monthly_data/data_for_month_19800430.csv read successfully.\n",
      "File monthly_data/data_for_month_19800530.csv read successfully.\n",
      "File monthly_data/data_for_month_19800630.csv read successfully.\n",
      "File monthly_data/data_for_month_19800731.csv read successfully.\n",
      "File monthly_data/data_for_month_19800829.csv read successfully.\n",
      "File monthly_data/data_for_month_19800930.csv read successfully.\n",
      "File monthly_data/data_for_month_19801031.csv read successfully.\n",
      "File monthly_data/data_for_month_19801128.csv read successfully.\n",
      "File monthly_data/data_for_month_19801231.csv read successfully.\n",
      "File monthly_data/data_for_month_19810130.csv read successfully.\n",
      "File monthly_data/data_for_month_19810227.csv read successfully.\n",
      "File monthly_data/data_for_month_19810331.csv read successfully.\n",
      "File monthly_data/data_for_month_19810430.csv read successfully.\n",
      "File monthly_data/data_for_month_19810529.csv read successfully.\n",
      "File monthly_data/data_for_month_19810630.csv read successfully.\n",
      "File monthly_data/data_for_month_19810731.csv read successfully.\n",
      "File monthly_data/data_for_month_19810831.csv read successfully.\n",
      "File monthly_data/data_for_month_19810930.csv read successfully.\n",
      "File monthly_data/data_for_month_19811030.csv read successfully.\n",
      "File monthly_data/data_for_month_19811130.csv read successfully.\n",
      "File monthly_data/data_for_month_19811231.csv read successfully.\n",
      "File monthly_data/data_for_month_19820129.csv read successfully.\n",
      "File monthly_data/data_for_month_19820226.csv read successfully.\n",
      "File monthly_data/data_for_month_19820331.csv read successfully.\n",
      "File monthly_data/data_for_month_19820430.csv read successfully.\n",
      "File monthly_data/data_for_month_19820528.csv read successfully.\n",
      "File monthly_data/data_for_month_19820630.csv read successfully.\n",
      "File monthly_data/data_for_month_19820730.csv read successfully.\n",
      "File monthly_data/data_for_month_19820831.csv read successfully.\n",
      "File monthly_data/data_for_month_19820930.csv read successfully.\n",
      "File monthly_data/data_for_month_19821029.csv read successfully.\n",
      "File monthly_data/data_for_month_19821130.csv read successfully.\n",
      "File monthly_data/data_for_month_19821231.csv read successfully.\n",
      "File monthly_data/data_for_month_19830131.csv read successfully.\n",
      "File monthly_data/data_for_month_19830228.csv read successfully.\n",
      "File monthly_data/data_for_month_19830331.csv read successfully.\n",
      "File monthly_data/data_for_month_19830429.csv read successfully.\n",
      "File monthly_data/data_for_month_19830531.csv read successfully.\n",
      "File monthly_data/data_for_month_19830630.csv read successfully.\n",
      "File monthly_data/data_for_month_19830729.csv read successfully.\n",
      "File monthly_data/data_for_month_19830831.csv read successfully.\n",
      "File monthly_data/data_for_month_19830930.csv read successfully.\n",
      "File monthly_data/data_for_month_19831031.csv read successfully.\n",
      "File monthly_data/data_for_month_19831130.csv read successfully.\n",
      "File monthly_data/data_for_month_19831230.csv read successfully.\n",
      "File monthly_data/data_for_month_19840131.csv read successfully.\n",
      "File monthly_data/data_for_month_19840229.csv read successfully.\n",
      "File monthly_data/data_for_month_19840330.csv read successfully.\n",
      "File monthly_data/data_for_month_19840430.csv read successfully.\n",
      "File monthly_data/data_for_month_19840531.csv read successfully.\n",
      "File monthly_data/data_for_month_19840629.csv read successfully.\n",
      "File monthly_data/data_for_month_19840731.csv read successfully.\n",
      "File monthly_data/data_for_month_19840831.csv read successfully.\n",
      "File monthly_data/data_for_month_19840928.csv read successfully.\n",
      "File monthly_data/data_for_month_19841031.csv read successfully.\n",
      "File monthly_data/data_for_month_19841130.csv read successfully.\n",
      "File monthly_data/data_for_month_19841231.csv read successfully.\n",
      "File monthly_data/data_for_month_19850131.csv read successfully.\n",
      "File monthly_data/data_for_month_19850228.csv read successfully.\n",
      "File monthly_data/data_for_month_19850329.csv read successfully.\n",
      "File monthly_data/data_for_month_19850430.csv read successfully.\n",
      "File monthly_data/data_for_month_19850531.csv read successfully.\n",
      "File monthly_data/data_for_month_19850628.csv read successfully.\n",
      "File monthly_data/data_for_month_19850731.csv read successfully.\n",
      "File monthly_data/data_for_month_19850830.csv read successfully.\n",
      "File monthly_data/data_for_month_19850930.csv read successfully.\n",
      "File monthly_data/data_for_month_19851031.csv read successfully.\n",
      "File monthly_data/data_for_month_19851129.csv read successfully.\n",
      "File monthly_data/data_for_month_19851231.csv read successfully.\n",
      "File monthly_data/data_for_month_19860131.csv read successfully.\n",
      "File monthly_data/data_for_month_19860228.csv read successfully.\n",
      "File monthly_data/data_for_month_19860331.csv read successfully.\n",
      "File monthly_data/data_for_month_19860430.csv read successfully.\n",
      "File monthly_data/data_for_month_19860530.csv read successfully.\n",
      "File monthly_data/data_for_month_19860630.csv read successfully.\n",
      "File monthly_data/data_for_month_19860731.csv read successfully.\n",
      "File monthly_data/data_for_month_19860829.csv read successfully.\n",
      "File monthly_data/data_for_month_19860930.csv read successfully.\n",
      "File monthly_data/data_for_month_19861031.csv read successfully.\n",
      "File monthly_data/data_for_month_19861128.csv read successfully.\n",
      "File monthly_data/data_for_month_19861231.csv read successfully.\n",
      "File monthly_data/data_for_month_19870130.csv read successfully.\n",
      "File monthly_data/data_for_month_19870227.csv read successfully.\n",
      "File monthly_data/data_for_month_19870331.csv read successfully.\n",
      "File monthly_data/data_for_month_19870430.csv read successfully.\n",
      "File monthly_data/data_for_month_19870529.csv read successfully.\n",
      "File monthly_data/data_for_month_19870630.csv read successfully.\n",
      "File monthly_data/data_for_month_19870731.csv read successfully.\n",
      "File monthly_data/data_for_month_19870831.csv read successfully.\n",
      "File monthly_data/data_for_month_19870930.csv read successfully.\n",
      "File monthly_data/data_for_month_19871030.csv read successfully.\n",
      "File monthly_data/data_for_month_19871130.csv read successfully.\n",
      "File monthly_data/data_for_month_19871231.csv read successfully.\n",
      "File monthly_data/data_for_month_19880129.csv read successfully.\n",
      "File monthly_data/data_for_month_19880229.csv read successfully.\n",
      "File monthly_data/data_for_month_19880331.csv read successfully.\n",
      "File monthly_data/data_for_month_19880429.csv read successfully.\n",
      "File monthly_data/data_for_month_19880531.csv read successfully.\n",
      "File monthly_data/data_for_month_19880630.csv read successfully.\n",
      "File monthly_data/data_for_month_19880729.csv read successfully.\n",
      "File monthly_data/data_for_month_19880831.csv read successfully.\n",
      "File monthly_data/data_for_month_19880930.csv read successfully.\n",
      "File monthly_data/data_for_month_19881031.csv read successfully.\n",
      "File monthly_data/data_for_month_19881130.csv read successfully.\n",
      "File monthly_data/data_for_month_19881230.csv read successfully.\n",
      "File monthly_data/data_for_month_19890131.csv read successfully.\n",
      "File monthly_data/data_for_month_19890228.csv read successfully.\n",
      "File monthly_data/data_for_month_19890331.csv read successfully.\n",
      "File monthly_data/data_for_month_19890428.csv read successfully.\n",
      "File monthly_data/data_for_month_19890531.csv read successfully.\n",
      "File monthly_data/data_for_month_19890630.csv read successfully.\n",
      "File monthly_data/data_for_month_19890731.csv read successfully.\n",
      "File monthly_data/data_for_month_19890831.csv read successfully.\n",
      "File monthly_data/data_for_month_19890929.csv read successfully.\n",
      "File monthly_data/data_for_month_19891031.csv read successfully.\n",
      "File monthly_data/data_for_month_19891130.csv read successfully.\n",
      "File monthly_data/data_for_month_19891229.csv read successfully.\n",
      "File monthly_data/data_for_month_19900131.csv read successfully.\n",
      "File monthly_data/data_for_month_19900228.csv read successfully.\n",
      "File monthly_data/data_for_month_19900330.csv read successfully.\n",
      "File monthly_data/data_for_month_19900430.csv read successfully.\n",
      "File monthly_data/data_for_month_19900531.csv read successfully.\n",
      "File monthly_data/data_for_month_19900629.csv read successfully.\n",
      "File monthly_data/data_for_month_19900731.csv read successfully.\n",
      "File monthly_data/data_for_month_19900831.csv read successfully.\n",
      "File monthly_data/data_for_month_19900928.csv read successfully.\n",
      "File monthly_data/data_for_month_19901031.csv read successfully.\n",
      "File monthly_data/data_for_month_19901130.csv read successfully.\n",
      "File monthly_data/data_for_month_19901231.csv read successfully.\n",
      "File monthly_data/data_for_month_19910131.csv read successfully.\n",
      "File monthly_data/data_for_month_19910228.csv read successfully.\n",
      "File monthly_data/data_for_month_19910328.csv read successfully.\n",
      "File monthly_data/data_for_month_19910430.csv read successfully.\n",
      "File monthly_data/data_for_month_19910531.csv read successfully.\n",
      "File monthly_data/data_for_month_19910628.csv read successfully.\n",
      "File monthly_data/data_for_month_19910731.csv read successfully.\n",
      "File monthly_data/data_for_month_19910830.csv read successfully.\n",
      "File monthly_data/data_for_month_19910930.csv read successfully.\n",
      "File monthly_data/data_for_month_19911031.csv read successfully.\n",
      "File monthly_data/data_for_month_19911129.csv read successfully.\n",
      "File monthly_data/data_for_month_19911231.csv read successfully.\n",
      "File monthly_data/data_for_month_19920131.csv read successfully.\n",
      "File monthly_data/data_for_month_19920228.csv read successfully.\n",
      "File monthly_data/data_for_month_19920331.csv read successfully.\n",
      "File monthly_data/data_for_month_19920430.csv read successfully.\n",
      "File monthly_data/data_for_month_19920529.csv read successfully.\n",
      "File monthly_data/data_for_month_19920630.csv read successfully.\n",
      "File monthly_data/data_for_month_19920731.csv read successfully.\n",
      "File monthly_data/data_for_month_19920831.csv read successfully.\n",
      "File monthly_data/data_for_month_19920930.csv read successfully.\n",
      "File monthly_data/data_for_month_19921030.csv read successfully.\n",
      "File monthly_data/data_for_month_19921130.csv read successfully.\n",
      "File monthly_data/data_for_month_19921231.csv read successfully.\n",
      "File monthly_data/data_for_month_19930129.csv read successfully.\n",
      "File monthly_data/data_for_month_19930226.csv read successfully.\n",
      "File monthly_data/data_for_month_19930331.csv read successfully.\n",
      "File monthly_data/data_for_month_19930430.csv read successfully.\n",
      "File monthly_data/data_for_month_19930528.csv read successfully.\n",
      "File monthly_data/data_for_month_19930630.csv read successfully.\n",
      "File monthly_data/data_for_month_19930730.csv read successfully.\n",
      "File monthly_data/data_for_month_19930831.csv read successfully.\n",
      "File monthly_data/data_for_month_19930930.csv read successfully.\n",
      "File monthly_data/data_for_month_19931029.csv read successfully.\n",
      "File monthly_data/data_for_month_19931130.csv read successfully.\n",
      "File monthly_data/data_for_month_19931231.csv read successfully.\n",
      "File monthly_data/data_for_month_19940131.csv read successfully.\n",
      "File monthly_data/data_for_month_19940228.csv read successfully.\n",
      "File monthly_data/data_for_month_19940331.csv read successfully.\n",
      "File monthly_data/data_for_month_19940429.csv read successfully.\n",
      "File monthly_data/data_for_month_19940531.csv read successfully.\n",
      "File monthly_data/data_for_month_19940630.csv read successfully.\n",
      "File monthly_data/data_for_month_19940729.csv read successfully.\n",
      "File monthly_data/data_for_month_19940831.csv read successfully.\n",
      "File monthly_data/data_for_month_19940930.csv read successfully.\n",
      "File monthly_data/data_for_month_19941031.csv read successfully.\n",
      "File monthly_data/data_for_month_19941130.csv read successfully.\n",
      "File monthly_data/data_for_month_19941230.csv read successfully.\n",
      "File monthly_data/data_for_month_19950131.csv read successfully.\n",
      "File monthly_data/data_for_month_19950228.csv read successfully.\n",
      "File monthly_data/data_for_month_19950331.csv read successfully.\n",
      "File monthly_data/data_for_month_19950428.csv read successfully.\n",
      "File monthly_data/data_for_month_19950531.csv read successfully.\n",
      "File monthly_data/data_for_month_19950630.csv read successfully.\n",
      "File monthly_data/data_for_month_19950731.csv read successfully.\n",
      "File monthly_data/data_for_month_19950831.csv read successfully.\n",
      "File monthly_data/data_for_month_19950929.csv read successfully.\n",
      "File monthly_data/data_for_month_19951031.csv read successfully.\n",
      "File monthly_data/data_for_month_19951130.csv read successfully.\n",
      "File monthly_data/data_for_month_19951229.csv read successfully.\n",
      "File monthly_data/data_for_month_19960131.csv read successfully.\n",
      "File monthly_data/data_for_month_19960229.csv read successfully.\n",
      "File monthly_data/data_for_month_19960329.csv read successfully.\n",
      "File monthly_data/data_for_month_19960430.csv read successfully.\n",
      "File monthly_data/data_for_month_19960531.csv read successfully.\n",
      "File monthly_data/data_for_month_19960628.csv read successfully.\n",
      "File monthly_data/data_for_month_19960731.csv read successfully.\n",
      "File monthly_data/data_for_month_19960830.csv read successfully.\n",
      "File monthly_data/data_for_month_19960930.csv read successfully.\n",
      "File monthly_data/data_for_month_19961031.csv read successfully.\n",
      "File monthly_data/data_for_month_19961129.csv read successfully.\n",
      "File monthly_data/data_for_month_19961231.csv read successfully.\n",
      "File monthly_data/data_for_month_19970131.csv read successfully.\n",
      "File monthly_data/data_for_month_19970228.csv read successfully.\n",
      "File monthly_data/data_for_month_19970331.csv read successfully.\n",
      "File monthly_data/data_for_month_19970430.csv read successfully.\n",
      "File monthly_data/data_for_month_19970530.csv read successfully.\n",
      "File monthly_data/data_for_month_19970630.csv read successfully.\n",
      "File monthly_data/data_for_month_19970731.csv read successfully.\n",
      "File monthly_data/data_for_month_19970829.csv read successfully.\n",
      "File monthly_data/data_for_month_19970930.csv read successfully.\n",
      "File monthly_data/data_for_month_19971031.csv read successfully.\n",
      "File monthly_data/data_for_month_19971128.csv read successfully.\n",
      "File monthly_data/data_for_month_19971231.csv read successfully.\n",
      "File monthly_data/data_for_month_19980130.csv read successfully.\n",
      "File monthly_data/data_for_month_19980227.csv read successfully.\n",
      "File monthly_data/data_for_month_19980331.csv read successfully.\n",
      "File monthly_data/data_for_month_19980430.csv read successfully.\n",
      "File monthly_data/data_for_month_19980529.csv read successfully.\n",
      "File monthly_data/data_for_month_19980630.csv read successfully.\n",
      "File monthly_data/data_for_month_19980731.csv read successfully.\n",
      "File monthly_data/data_for_month_19980831.csv read successfully.\n",
      "File monthly_data/data_for_month_19980930.csv read successfully.\n",
      "File monthly_data/data_for_month_19981030.csv read successfully.\n",
      "File monthly_data/data_for_month_19981130.csv read successfully.\n",
      "File monthly_data/data_for_month_19981231.csv read successfully.\n",
      "File monthly_data/data_for_month_19990129.csv read successfully.\n",
      "File monthly_data/data_for_month_19990226.csv read successfully.\n",
      "File monthly_data/data_for_month_19990331.csv read successfully.\n",
      "File monthly_data/data_for_month_19990430.csv read successfully.\n",
      "File monthly_data/data_for_month_19990528.csv read successfully.\n",
      "File monthly_data/data_for_month_19990630.csv read successfully.\n",
      "File monthly_data/data_for_month_19990730.csv read successfully.\n",
      "File monthly_data/data_for_month_19990831.csv read successfully.\n",
      "File monthly_data/data_for_month_19990930.csv read successfully.\n",
      "File monthly_data/data_for_month_19991029.csv read successfully.\n",
      "File monthly_data/data_for_month_19991130.csv read successfully.\n",
      "File monthly_data/data_for_month_19991231.csv read successfully.\n",
      "File monthly_data/data_for_month_20000131.csv read successfully.\n",
      "File monthly_data/data_for_month_20000229.csv read successfully.\n",
      "File monthly_data/data_for_month_20000331.csv read successfully.\n",
      "File monthly_data/data_for_month_20000428.csv read successfully.\n",
      "File monthly_data/data_for_month_20000531.csv read successfully.\n",
      "File monthly_data/data_for_month_20000630.csv read successfully.\n",
      "File monthly_data/data_for_month_20000731.csv read successfully.\n",
      "File monthly_data/data_for_month_20000831.csv read successfully.\n",
      "File monthly_data/data_for_month_20000929.csv read successfully.\n",
      "File monthly_data/data_for_month_20001031.csv read successfully.\n",
      "File monthly_data/data_for_month_20001130.csv read successfully.\n",
      "File monthly_data/data_for_month_20001229.csv read successfully.\n",
      "File monthly_data/data_for_month_20010131.csv read successfully.\n",
      "File monthly_data/data_for_month_20010228.csv read successfully.\n",
      "File monthly_data/data_for_month_20010330.csv read successfully.\n",
      "File monthly_data/data_for_month_20010430.csv read successfully.\n",
      "File monthly_data/data_for_month_20010531.csv read successfully.\n",
      "File monthly_data/data_for_month_20010629.csv read successfully.\n",
      "File monthly_data/data_for_month_20010731.csv read successfully.\n",
      "File monthly_data/data_for_month_20010831.csv read successfully.\n",
      "File monthly_data/data_for_month_20010928.csv read successfully.\n",
      "File monthly_data/data_for_month_20011031.csv read successfully.\n",
      "File monthly_data/data_for_month_20011130.csv read successfully.\n",
      "File monthly_data/data_for_month_20011231.csv read successfully.\n",
      "File monthly_data/data_for_month_20020131.csv read successfully.\n",
      "File monthly_data/data_for_month_20020228.csv read successfully.\n",
      "File monthly_data/data_for_month_20020328.csv read successfully.\n",
      "File monthly_data/data_for_month_20020430.csv read successfully.\n",
      "File monthly_data/data_for_month_20020531.csv read successfully.\n",
      "File monthly_data/data_for_month_20020628.csv read successfully.\n",
      "File monthly_data/data_for_month_20020731.csv read successfully.\n",
      "File monthly_data/data_for_month_20020830.csv read successfully.\n",
      "File monthly_data/data_for_month_20020930.csv read successfully.\n",
      "File monthly_data/data_for_month_20021031.csv read successfully.\n",
      "File monthly_data/data_for_month_20021129.csv read successfully.\n",
      "File monthly_data/data_for_month_20021231.csv read successfully.\n",
      "File monthly_data/data_for_month_20030131.csv read successfully.\n",
      "File monthly_data/data_for_month_20030228.csv read successfully.\n",
      "File monthly_data/data_for_month_20030331.csv read successfully.\n",
      "File monthly_data/data_for_month_20030430.csv read successfully.\n",
      "File monthly_data/data_for_month_20030530.csv read successfully.\n",
      "File monthly_data/data_for_month_20030630.csv read successfully.\n",
      "File monthly_data/data_for_month_20030731.csv read successfully.\n",
      "File monthly_data/data_for_month_20030829.csv read successfully.\n",
      "File monthly_data/data_for_month_20030930.csv read successfully.\n",
      "File monthly_data/data_for_month_20031031.csv read successfully.\n",
      "File monthly_data/data_for_month_20031128.csv read successfully.\n",
      "File monthly_data/data_for_month_20031231.csv read successfully.\n",
      "File monthly_data/data_for_month_20040130.csv read successfully.\n",
      "File monthly_data/data_for_month_20040227.csv read successfully.\n",
      "File monthly_data/data_for_month_20040331.csv read successfully.\n",
      "File monthly_data/data_for_month_20040430.csv read successfully.\n",
      "File monthly_data/data_for_month_20040528.csv read successfully.\n",
      "File monthly_data/data_for_month_20040630.csv read successfully.\n",
      "File monthly_data/data_for_month_20040730.csv read successfully.\n",
      "File monthly_data/data_for_month_20040831.csv read successfully.\n",
      "File monthly_data/data_for_month_20040930.csv read successfully.\n",
      "File monthly_data/data_for_month_20041029.csv read successfully.\n",
      "File monthly_data/data_for_month_20041130.csv read successfully.\n",
      "File monthly_data/data_for_month_20041231.csv read successfully.\n",
      "File monthly_data/data_for_month_20050131.csv read successfully.\n",
      "File monthly_data/data_for_month_20050228.csv read successfully.\n",
      "File monthly_data/data_for_month_20050331.csv read successfully.\n",
      "File monthly_data/data_for_month_20050429.csv read successfully.\n",
      "File monthly_data/data_for_month_20050531.csv read successfully.\n",
      "File monthly_data/data_for_month_20050630.csv read successfully.\n",
      "File monthly_data/data_for_month_20050729.csv read successfully.\n",
      "File monthly_data/data_for_month_20050831.csv read successfully.\n",
      "File monthly_data/data_for_month_20050930.csv read successfully.\n",
      "File monthly_data/data_for_month_20051031.csv read successfully.\n",
      "File monthly_data/data_for_month_20051130.csv read successfully.\n",
      "File monthly_data/data_for_month_20051230.csv read successfully.\n",
      "File monthly_data/data_for_month_20060131.csv read successfully.\n",
      "File monthly_data/data_for_month_20060228.csv read successfully.\n",
      "File monthly_data/data_for_month_20060331.csv read successfully.\n",
      "File monthly_data/data_for_month_20060428.csv read successfully.\n",
      "File monthly_data/data_for_month_20060531.csv read successfully.\n",
      "File monthly_data/data_for_month_20060630.csv read successfully.\n",
      "File monthly_data/data_for_month_20060731.csv read successfully.\n",
      "File monthly_data/data_for_month_20060831.csv read successfully.\n",
      "File monthly_data/data_for_month_20060929.csv read successfully.\n",
      "File monthly_data/data_for_month_20061031.csv read successfully.\n",
      "File monthly_data/data_for_month_20061130.csv read successfully.\n",
      "File monthly_data/data_for_month_20061229.csv read successfully.\n",
      "File monthly_data/data_for_month_20070131.csv read successfully.\n",
      "File monthly_data/data_for_month_20070228.csv read successfully.\n",
      "File monthly_data/data_for_month_20070330.csv read successfully.\n",
      "File monthly_data/data_for_month_20070430.csv read successfully.\n",
      "File monthly_data/data_for_month_20070531.csv read successfully.\n",
      "File monthly_data/data_for_month_20070629.csv read successfully.\n",
      "File monthly_data/data_for_month_20070731.csv read successfully.\n",
      "File monthly_data/data_for_month_20070831.csv read successfully.\n",
      "File monthly_data/data_for_month_20070928.csv read successfully.\n",
      "File monthly_data/data_for_month_20071031.csv read successfully.\n",
      "File monthly_data/data_for_month_20071130.csv read successfully.\n",
      "File monthly_data/data_for_month_20071231.csv read successfully.\n",
      "File monthly_data/data_for_month_20080131.csv read successfully.\n",
      "File monthly_data/data_for_month_20080229.csv read successfully.\n",
      "File monthly_data/data_for_month_20080331.csv read successfully.\n",
      "File monthly_data/data_for_month_20080430.csv read successfully.\n",
      "File monthly_data/data_for_month_20080530.csv read successfully.\n",
      "File monthly_data/data_for_month_20080630.csv read successfully.\n",
      "File monthly_data/data_for_month_20080731.csv read successfully.\n",
      "File monthly_data/data_for_month_20080829.csv read successfully.\n",
      "File monthly_data/data_for_month_20080930.csv read successfully.\n",
      "File monthly_data/data_for_month_20081031.csv read successfully.\n",
      "File monthly_data/data_for_month_20081128.csv read successfully.\n",
      "File monthly_data/data_for_month_20081231.csv read successfully.\n",
      "File monthly_data/data_for_month_20090130.csv read successfully.\n",
      "File monthly_data/data_for_month_20090227.csv read successfully.\n",
      "File monthly_data/data_for_month_20090331.csv read successfully.\n",
      "File monthly_data/data_for_month_20090430.csv read successfully.\n",
      "File monthly_data/data_for_month_20090529.csv read successfully.\n",
      "File monthly_data/data_for_month_20090630.csv read successfully.\n",
      "File monthly_data/data_for_month_20090731.csv read successfully.\n",
      "File monthly_data/data_for_month_20090831.csv read successfully.\n",
      "File monthly_data/data_for_month_20090930.csv read successfully.\n",
      "File monthly_data/data_for_month_20091030.csv read successfully.\n",
      "File monthly_data/data_for_month_20091130.csv read successfully.\n",
      "File monthly_data/data_for_month_20091231.csv read successfully.\n",
      "File monthly_data/data_for_month_20100129.csv read successfully.\n",
      "File monthly_data/data_for_month_20100226.csv read successfully.\n",
      "File monthly_data/data_for_month_20100331.csv read successfully.\n",
      "File monthly_data/data_for_month_20100430.csv read successfully.\n",
      "File monthly_data/data_for_month_20100528.csv read successfully.\n",
      "File monthly_data/data_for_month_20100630.csv read successfully.\n",
      "File monthly_data/data_for_month_20100730.csv read successfully.\n",
      "File monthly_data/data_for_month_20100831.csv read successfully.\n",
      "File monthly_data/data_for_month_20100930.csv read successfully.\n",
      "File monthly_data/data_for_month_20101029.csv read successfully.\n",
      "File monthly_data/data_for_month_20101130.csv read successfully.\n",
      "File monthly_data/data_for_month_20101231.csv read successfully.\n",
      "File monthly_data/data_for_month_20110131.csv read successfully.\n",
      "File monthly_data/data_for_month_20110228.csv read successfully.\n",
      "File monthly_data/data_for_month_20110331.csv read successfully.\n",
      "File monthly_data/data_for_month_20110429.csv read successfully.\n",
      "File monthly_data/data_for_month_20110531.csv read successfully.\n",
      "File monthly_data/data_for_month_20110630.csv read successfully.\n",
      "File monthly_data/data_for_month_20110729.csv read successfully.\n",
      "File monthly_data/data_for_month_20110831.csv read successfully.\n",
      "File monthly_data/data_for_month_20110930.csv read successfully.\n",
      "File monthly_data/data_for_month_20111031.csv read successfully.\n",
      "File monthly_data/data_for_month_20111130.csv read successfully.\n",
      "File monthly_data/data_for_month_20111230.csv read successfully.\n",
      "File monthly_data/data_for_month_20120131.csv read successfully.\n",
      "File monthly_data/data_for_month_20120229.csv read successfully.\n",
      "File monthly_data/data_for_month_20120330.csv read successfully.\n",
      "File monthly_data/data_for_month_20120430.csv read successfully.\n",
      "File monthly_data/data_for_month_20120531.csv read successfully.\n",
      "File monthly_data/data_for_month_20120629.csv read successfully.\n",
      "File monthly_data/data_for_month_20120731.csv read successfully.\n",
      "File monthly_data/data_for_month_20120831.csv read successfully.\n",
      "File monthly_data/data_for_month_20120928.csv read successfully.\n",
      "File monthly_data/data_for_month_20121031.csv read successfully.\n",
      "File monthly_data/data_for_month_20121130.csv read successfully.\n",
      "File monthly_data/data_for_month_20121231.csv read successfully.\n",
      "File monthly_data/data_for_month_20130131.csv read successfully.\n",
      "File monthly_data/data_for_month_20130228.csv read successfully.\n",
      "File monthly_data/data_for_month_20130328.csv read successfully.\n",
      "File monthly_data/data_for_month_20130430.csv read successfully.\n",
      "File monthly_data/data_for_month_20130531.csv read successfully.\n",
      "File monthly_data/data_for_month_20130628.csv read successfully.\n",
      "File monthly_data/data_for_month_20130731.csv read successfully.\n",
      "File monthly_data/data_for_month_20130830.csv read successfully.\n",
      "File monthly_data/data_for_month_20130930.csv read successfully.\n",
      "File monthly_data/data_for_month_20131031.csv read successfully.\n",
      "File monthly_data/data_for_month_20131129.csv read successfully.\n",
      "File monthly_data/data_for_month_20131231.csv read successfully.\n",
      "File monthly_data/data_for_month_20140131.csv read successfully.\n",
      "File monthly_data/data_for_month_20140228.csv read successfully.\n",
      "File monthly_data/data_for_month_20140331.csv read successfully.\n",
      "File monthly_data/data_for_month_20140430.csv read successfully.\n",
      "File monthly_data/data_for_month_20140530.csv read successfully.\n",
      "File monthly_data/data_for_month_20140630.csv read successfully.\n",
      "File monthly_data/data_for_month_20140731.csv read successfully.\n",
      "File monthly_data/data_for_month_20140829.csv read successfully.\n",
      "File monthly_data/data_for_month_20140930.csv read successfully.\n",
      "File monthly_data/data_for_month_20141031.csv read successfully.\n",
      "File monthly_data/data_for_month_20141128.csv read successfully.\n",
      "File monthly_data/data_for_month_20141231.csv read successfully.\n",
      "File monthly_data/data_for_month_20150130.csv read successfully.\n",
      "File monthly_data/data_for_month_20150227.csv read successfully.\n",
      "File monthly_data/data_for_month_20150331.csv read successfully.\n",
      "File monthly_data/data_for_month_20150430.csv read successfully.\n",
      "File monthly_data/data_for_month_20150529.csv read successfully.\n",
      "File monthly_data/data_for_month_20150630.csv read successfully.\n",
      "File monthly_data/data_for_month_20150731.csv read successfully.\n",
      "File monthly_data/data_for_month_20150831.csv read successfully.\n",
      "File monthly_data/data_for_month_20150930.csv read successfully.\n",
      "File monthly_data/data_for_month_20151030.csv read successfully.\n",
      "File monthly_data/data_for_month_20151130.csv read successfully.\n",
      "File monthly_data/data_for_month_20151231.csv read successfully.\n",
      "File monthly_data/data_for_month_20160129.csv read successfully.\n",
      "File monthly_data/data_for_month_20160229.csv read successfully.\n",
      "File monthly_data/data_for_month_20160331.csv read successfully.\n",
      "File monthly_data/data_for_month_20160429.csv read successfully.\n",
      "File monthly_data/data_for_month_20160531.csv read successfully.\n",
      "File monthly_data/data_for_month_20160630.csv read successfully.\n",
      "File monthly_data/data_for_month_20160729.csv read successfully.\n",
      "File monthly_data/data_for_month_20160831.csv read successfully.\n",
      "File monthly_data/data_for_month_20160930.csv read successfully.\n",
      "File monthly_data/data_for_month_20161031.csv read successfully.\n",
      "File monthly_data/data_for_month_20161130.csv read successfully.\n",
      "File monthly_data/data_for_month_20161230.csv read successfully.\n",
      "File monthly_data/data_for_month_20170131.csv read successfully.\n",
      "File monthly_data/data_for_month_20170228.csv read successfully.\n",
      "File monthly_data/data_for_month_20170331.csv read successfully.\n",
      "File monthly_data/data_for_month_20170428.csv read successfully.\n",
      "File monthly_data/data_for_month_20170531.csv read successfully.\n",
      "File monthly_data/data_for_month_20170630.csv read successfully.\n",
      "File monthly_data/data_for_month_20170731.csv read successfully.\n",
      "File monthly_data/data_for_month_20170831.csv read successfully.\n",
      "File monthly_data/data_for_month_20170929.csv read successfully.\n",
      "File monthly_data/data_for_month_20171031.csv read successfully.\n",
      "File monthly_data/data_for_month_20171130.csv read successfully.\n",
      "File monthly_data/data_for_month_20171229.csv read successfully.\n",
      "File monthly_data/data_for_month_20180131.csv read successfully.\n",
      "File monthly_data/data_for_month_20180228.csv read successfully.\n",
      "File monthly_data/data_for_month_20180329.csv read successfully.\n",
      "File monthly_data/data_for_month_20180430.csv read successfully.\n",
      "File monthly_data/data_for_month_20180531.csv read successfully.\n",
      "File monthly_data/data_for_month_20180629.csv read successfully.\n",
      "File monthly_data/data_for_month_20180731.csv read successfully.\n",
      "File monthly_data/data_for_month_20180831.csv read successfully.\n",
      "File monthly_data/data_for_month_20180928.csv read successfully.\n",
      "File monthly_data/data_for_month_20181031.csv read successfully.\n",
      "File monthly_data/data_for_month_20181130.csv read successfully.\n",
      "File monthly_data/data_for_month_20181231.csv read successfully.\n",
      "File monthly_data/data_for_month_20190131.csv read successfully.\n",
      "File monthly_data/data_for_month_20190228.csv read successfully.\n",
      "File monthly_data/data_for_month_20190329.csv read successfully.\n",
      "File monthly_data/data_for_month_20190430.csv read successfully.\n",
      "File monthly_data/data_for_month_20190531.csv read successfully.\n",
      "File monthly_data/data_for_month_20190628.csv read successfully.\n",
      "File monthly_data/data_for_month_20190731.csv read successfully.\n",
      "File monthly_data/data_for_month_20190830.csv read successfully.\n",
      "File monthly_data/data_for_month_20190930.csv read successfully.\n",
      "File monthly_data/data_for_month_20191031.csv read successfully.\n",
      "File monthly_data/data_for_month_20191129.csv read successfully.\n",
      "File monthly_data/data_for_month_20191231.csv read successfully.\n",
      "File monthly_data/data_for_month_20200131.csv read successfully.\n",
      "File monthly_data/data_for_month_20200228.csv read successfully.\n",
      "File monthly_data/data_for_month_20200331.csv read successfully.\n",
      "File monthly_data/data_for_month_20200430.csv read successfully.\n",
      "File monthly_data/data_for_month_20200529.csv read successfully.\n",
      "File monthly_data/data_for_month_20200630.csv read successfully.\n",
      "File monthly_data/data_for_month_20200731.csv read successfully.\n",
      "File monthly_data/data_for_month_20200831.csv read successfully.\n",
      "File monthly_data/data_for_month_20200930.csv read successfully.\n",
      "File monthly_data/data_for_month_20201030.csv read successfully.\n",
      "File monthly_data/data_for_month_20201130.csv read successfully.\n",
      "File monthly_data/data_for_month_20201231.csv read successfully.\n"
     ]
    }
   ],
   "source": [
    "z_list, x_list,  returns_list, number_of_obs_list = get_data_to_matrix_pca_names_only(intersection, pca_cols, files_from_1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('characters_pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(z_list, f)\n",
    "# with open('factors_pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(x_list, f)\n",
    "# with open('returns_pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(returns_list, f)\n",
    "# with open('number_of_obs_pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(number_of_obs_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monthly_data/data_for_month_19691231.csv': 1441,\n",
       " 'monthly_data/data_for_month_19700130.csv': 1447,\n",
       " 'monthly_data/data_for_month_19700227.csv': 1452,\n",
       " 'monthly_data/data_for_month_19700331.csv': 1461,\n",
       " 'monthly_data/data_for_month_19700430.csv': 1476,\n",
       " 'monthly_data/data_for_month_19700529.csv': 1484,\n",
       " 'monthly_data/data_for_month_19700630.csv': 1488,\n",
       " 'monthly_data/data_for_month_19700731.csv': 1608,\n",
       " 'monthly_data/data_for_month_19700831.csv': 1614,\n",
       " 'monthly_data/data_for_month_19700930.csv': 1617,\n",
       " 'monthly_data/data_for_month_19701030.csv': 1623,\n",
       " 'monthly_data/data_for_month_19701130.csv': 1632,\n",
       " 'monthly_data/data_for_month_19701231.csv': 1628,\n",
       " 'monthly_data/data_for_month_19710129.csv': 1635,\n",
       " 'monthly_data/data_for_month_19710226.csv': 1643,\n",
       " 'monthly_data/data_for_month_19710331.csv': 1648,\n",
       " 'monthly_data/data_for_month_19710430.csv': 1662,\n",
       " 'monthly_data/data_for_month_19710528.csv': 1668,\n",
       " 'monthly_data/data_for_month_19710630.csv': 1667,\n",
       " 'monthly_data/data_for_month_19710730.csv': 1767,\n",
       " 'monthly_data/data_for_month_19710831.csv': 1775,\n",
       " 'monthly_data/data_for_month_19710930.csv': 1784,\n",
       " 'monthly_data/data_for_month_19711029.csv': 1791,\n",
       " 'monthly_data/data_for_month_19711130.csv': 1800,\n",
       " 'monthly_data/data_for_month_19711231.csv': 1800,\n",
       " 'monthly_data/data_for_month_19720131.csv': 1805,\n",
       " 'monthly_data/data_for_month_19720229.csv': 1807,\n",
       " 'monthly_data/data_for_month_19720330.csv': 1807,\n",
       " 'monthly_data/data_for_month_19720428.csv': 1816,\n",
       " 'monthly_data/data_for_month_19720531.csv': 1817,\n",
       " 'monthly_data/data_for_month_19720630.csv': 1818,\n",
       " 'monthly_data/data_for_month_19720731.csv': 1872,\n",
       " 'monthly_data/data_for_month_19720831.csv': 1866,\n",
       " 'monthly_data/data_for_month_19720929.csv': 1862,\n",
       " 'monthly_data/data_for_month_19721031.csv': 1867,\n",
       " 'monthly_data/data_for_month_19721130.csv': 1874,\n",
       " 'monthly_data/data_for_month_19721229.csv': 1879,\n",
       " 'monthly_data/data_for_month_19730131.csv': 1879,\n",
       " 'monthly_data/data_for_month_19730228.csv': 1878,\n",
       " 'monthly_data/data_for_month_19730330.csv': 1879,\n",
       " 'monthly_data/data_for_month_19730430.csv': 1884,\n",
       " 'monthly_data/data_for_month_19730531.csv': 1885,\n",
       " 'monthly_data/data_for_month_19730629.csv': 1887,\n",
       " 'monthly_data/data_for_month_19730731.csv': 1953,\n",
       " 'monthly_data/data_for_month_19730831.csv': 1958,\n",
       " 'monthly_data/data_for_month_19730928.csv': 1961,\n",
       " 'monthly_data/data_for_month_19731031.csv': 1966,\n",
       " 'monthly_data/data_for_month_19731130.csv': 1961,\n",
       " 'monthly_data/data_for_month_19731231.csv': 2041,\n",
       " 'monthly_data/data_for_month_19740131.csv': 2107,\n",
       " 'monthly_data/data_for_month_19740228.csv': 2262,\n",
       " 'monthly_data/data_for_month_19740329.csv': 2294,\n",
       " 'monthly_data/data_for_month_19740430.csv': 2373,\n",
       " 'monthly_data/data_for_month_19740531.csv': 2414,\n",
       " 'monthly_data/data_for_month_19740628.csv': 2440,\n",
       " 'monthly_data/data_for_month_19740731.csv': 2927,\n",
       " 'monthly_data/data_for_month_19740830.csv': 2939,\n",
       " 'monthly_data/data_for_month_19740930.csv': 2930,\n",
       " 'monthly_data/data_for_month_19741031.csv': 2947,\n",
       " 'monthly_data/data_for_month_19741129.csv': 2938,\n",
       " 'monthly_data/data_for_month_19741231.csv': 2934,\n",
       " 'monthly_data/data_for_month_19750131.csv': 2944,\n",
       " 'monthly_data/data_for_month_19750228.csv': 2950,\n",
       " 'monthly_data/data_for_month_19750331.csv': 2959,\n",
       " 'monthly_data/data_for_month_19750430.csv': 2981,\n",
       " 'monthly_data/data_for_month_19750530.csv': 2978,\n",
       " 'monthly_data/data_for_month_19750630.csv': 2979,\n",
       " 'monthly_data/data_for_month_19750731.csv': 3166,\n",
       " 'monthly_data/data_for_month_19750829.csv': 3168,\n",
       " 'monthly_data/data_for_month_19750930.csv': 3158,\n",
       " 'monthly_data/data_for_month_19751031.csv': 3175,\n",
       " 'monthly_data/data_for_month_19751128.csv': 3171,\n",
       " 'monthly_data/data_for_month_19751231.csv': 3177,\n",
       " 'monthly_data/data_for_month_19760130.csv': 3120,\n",
       " 'monthly_data/data_for_month_19760227.csv': 3180,\n",
       " 'monthly_data/data_for_month_19760331.csv': 3145,\n",
       " 'monthly_data/data_for_month_19760430.csv': 3170,\n",
       " 'monthly_data/data_for_month_19760528.csv': 3168,\n",
       " 'monthly_data/data_for_month_19760630.csv': 3165,\n",
       " 'monthly_data/data_for_month_19760730.csv': 3233,\n",
       " 'monthly_data/data_for_month_19760831.csv': 2996,\n",
       " 'monthly_data/data_for_month_19760930.csv': 3198,\n",
       " 'monthly_data/data_for_month_19761029.csv': 3191,\n",
       " 'monthly_data/data_for_month_19761130.csv': 3192,\n",
       " 'monthly_data/data_for_month_19761231.csv': 3191,\n",
       " 'monthly_data/data_for_month_19770131.csv': 3159,\n",
       " 'monthly_data/data_for_month_19770228.csv': 3179,\n",
       " 'monthly_data/data_for_month_19770331.csv': 3169,\n",
       " 'monthly_data/data_for_month_19770429.csv': 3166,\n",
       " 'monthly_data/data_for_month_19770531.csv': 3159,\n",
       " 'monthly_data/data_for_month_19770630.csv': 3146,\n",
       " 'monthly_data/data_for_month_19770729.csv': 3176,\n",
       " 'monthly_data/data_for_month_19770831.csv': 3171,\n",
       " 'monthly_data/data_for_month_19770930.csv': 3155,\n",
       " 'monthly_data/data_for_month_19771031.csv': 3145,\n",
       " 'monthly_data/data_for_month_19771130.csv': 3139,\n",
       " 'monthly_data/data_for_month_19771230.csv': 3125,\n",
       " 'monthly_data/data_for_month_19780131.csv': 3109,\n",
       " 'monthly_data/data_for_month_19780228.csv': 3088,\n",
       " 'monthly_data/data_for_month_19780331.csv': 3086,\n",
       " 'monthly_data/data_for_month_19780428.csv': 3084,\n",
       " 'monthly_data/data_for_month_19780531.csv': 3079,\n",
       " 'monthly_data/data_for_month_19780630.csv': 3058,\n",
       " 'monthly_data/data_for_month_19780731.csv': 3100,\n",
       " 'monthly_data/data_for_month_19780831.csv': 3101,\n",
       " 'monthly_data/data_for_month_19780929.csv': 3067,\n",
       " 'monthly_data/data_for_month_19781031.csv': 3053,\n",
       " 'monthly_data/data_for_month_19781130.csv': 3035,\n",
       " 'monthly_data/data_for_month_19781229.csv': 3028,\n",
       " 'monthly_data/data_for_month_19790131.csv': 3018,\n",
       " 'monthly_data/data_for_month_19790228.csv': 2993,\n",
       " 'monthly_data/data_for_month_19790330.csv': 2985,\n",
       " 'monthly_data/data_for_month_19790430.csv': 2996,\n",
       " 'monthly_data/data_for_month_19790531.csv': 2984,\n",
       " 'monthly_data/data_for_month_19790629.csv': 2976,\n",
       " 'monthly_data/data_for_month_19790731.csv': 2995,\n",
       " 'monthly_data/data_for_month_19790831.csv': 3001,\n",
       " 'monthly_data/data_for_month_19790928.csv': 2987,\n",
       " 'monthly_data/data_for_month_19791031.csv': 2979,\n",
       " 'monthly_data/data_for_month_19791130.csv': 2982,\n",
       " 'monthly_data/data_for_month_19791231.csv': 2984,\n",
       " 'monthly_data/data_for_month_19800131.csv': 2982,\n",
       " 'monthly_data/data_for_month_19800229.csv': 2961,\n",
       " 'monthly_data/data_for_month_19800331.csv': 2957,\n",
       " 'monthly_data/data_for_month_19800430.csv': 2979,\n",
       " 'monthly_data/data_for_month_19800530.csv': 2982,\n",
       " 'monthly_data/data_for_month_19800630.csv': 2978,\n",
       " 'monthly_data/data_for_month_19800731.csv': 3055,\n",
       " 'monthly_data/data_for_month_19800829.csv': 3068,\n",
       " 'monthly_data/data_for_month_19800930.csv': 3055,\n",
       " 'monthly_data/data_for_month_19801031.csv': 3062,\n",
       " 'monthly_data/data_for_month_19801128.csv': 3056,\n",
       " 'monthly_data/data_for_month_19801231.csv': 3074,\n",
       " 'monthly_data/data_for_month_19810130.csv': 3099,\n",
       " 'monthly_data/data_for_month_19810227.csv': 3092,\n",
       " 'monthly_data/data_for_month_19810331.csv': 3095,\n",
       " 'monthly_data/data_for_month_19810430.csv': 3113,\n",
       " 'monthly_data/data_for_month_19810529.csv': 3119,\n",
       " 'monthly_data/data_for_month_19810630.csv': 3123,\n",
       " 'monthly_data/data_for_month_19810731.csv': 3267,\n",
       " 'monthly_data/data_for_month_19810831.csv': 3255,\n",
       " 'monthly_data/data_for_month_19810930.csv': 3225,\n",
       " 'monthly_data/data_for_month_19811030.csv': 3238,\n",
       " 'monthly_data/data_for_month_19811130.csv': 3223,\n",
       " 'monthly_data/data_for_month_19811231.csv': 3228,\n",
       " 'monthly_data/data_for_month_19820129.csv': 3232,\n",
       " 'monthly_data/data_for_month_19820226.csv': 3226,\n",
       " 'monthly_data/data_for_month_19820331.csv': 3218,\n",
       " 'monthly_data/data_for_month_19820430.csv': 3217,\n",
       " 'monthly_data/data_for_month_19820528.csv': 3216,\n",
       " 'monthly_data/data_for_month_19820630.csv': 3208,\n",
       " 'monthly_data/data_for_month_19820730.csv': 3316,\n",
       " 'monthly_data/data_for_month_19820831.csv': 3323,\n",
       " 'monthly_data/data_for_month_19820930.csv': 3299,\n",
       " 'monthly_data/data_for_month_19821029.csv': 3306,\n",
       " 'monthly_data/data_for_month_19821130.csv': 3303,\n",
       " 'monthly_data/data_for_month_19821231.csv': 3379,\n",
       " 'monthly_data/data_for_month_19830131.csv': 3467,\n",
       " 'monthly_data/data_for_month_19830228.csv': 3457,\n",
       " 'monthly_data/data_for_month_19830331.csv': 3476,\n",
       " 'monthly_data/data_for_month_19830429.csv': 3516,\n",
       " 'monthly_data/data_for_month_19830531.csv': 3520,\n",
       " 'monthly_data/data_for_month_19830630.csv': 3522,\n",
       " 'monthly_data/data_for_month_19830729.csv': 3712,\n",
       " 'monthly_data/data_for_month_19830831.csv': 3716,\n",
       " 'monthly_data/data_for_month_19830930.csv': 3704,\n",
       " 'monthly_data/data_for_month_19831031.csv': 3726,\n",
       " 'monthly_data/data_for_month_19831130.csv': 3720,\n",
       " 'monthly_data/data_for_month_19831230.csv': 3727,\n",
       " 'monthly_data/data_for_month_19840131.csv': 3752,\n",
       " 'monthly_data/data_for_month_19840229.csv': 3726,\n",
       " 'monthly_data/data_for_month_19840330.csv': 3718,\n",
       " 'monthly_data/data_for_month_19840430.csv': 3741,\n",
       " 'monthly_data/data_for_month_19840531.csv': 3729,\n",
       " 'monthly_data/data_for_month_19840629.csv': 3726,\n",
       " 'monthly_data/data_for_month_19840731.csv': 3849,\n",
       " 'monthly_data/data_for_month_19840831.csv': 3842,\n",
       " 'monthly_data/data_for_month_19840928.csv': 3781,\n",
       " 'monthly_data/data_for_month_19841031.csv': 3766,\n",
       " 'monthly_data/data_for_month_19841130.csv': 3738,\n",
       " 'monthly_data/data_for_month_19841231.csv': 3704,\n",
       " 'monthly_data/data_for_month_19850131.csv': 3727,\n",
       " 'monthly_data/data_for_month_19850228.csv': 3697,\n",
       " 'monthly_data/data_for_month_19850329.csv': 3698,\n",
       " 'monthly_data/data_for_month_19850430.csv': 3733,\n",
       " 'monthly_data/data_for_month_19850531.csv': 3737,\n",
       " 'monthly_data/data_for_month_19850628.csv': 3731,\n",
       " 'monthly_data/data_for_month_19850731.csv': 3974,\n",
       " 'monthly_data/data_for_month_19850830.csv': 3980,\n",
       " 'monthly_data/data_for_month_19850930.csv': 3953,\n",
       " 'monthly_data/data_for_month_19851031.csv': 3978,\n",
       " 'monthly_data/data_for_month_19851129.csv': 3964,\n",
       " 'monthly_data/data_for_month_19851231.csv': 3958,\n",
       " 'monthly_data/data_for_month_19860131.csv': 3978,\n",
       " 'monthly_data/data_for_month_19860228.csv': 3964,\n",
       " 'monthly_data/data_for_month_19860331.csv': 3964,\n",
       " 'monthly_data/data_for_month_19860430.csv': 3975,\n",
       " 'monthly_data/data_for_month_19860530.csv': 3970,\n",
       " 'monthly_data/data_for_month_19860630.csv': 3961,\n",
       " 'monthly_data/data_for_month_19860731.csv': 4120,\n",
       " 'monthly_data/data_for_month_19860829.csv': 4090,\n",
       " 'monthly_data/data_for_month_19860930.csv': 4067,\n",
       " 'monthly_data/data_for_month_19861031.csv': 4042,\n",
       " 'monthly_data/data_for_month_19861128.csv': 3977,\n",
       " 'monthly_data/data_for_month_19861231.csv': 3955,\n",
       " 'monthly_data/data_for_month_19870130.csv': 3948,\n",
       " 'monthly_data/data_for_month_19870227.csv': 3930,\n",
       " 'monthly_data/data_for_month_19870331.csv': 3921,\n",
       " 'monthly_data/data_for_month_19870430.csv': 3930,\n",
       " 'monthly_data/data_for_month_19870529.csv': 3929,\n",
       " 'monthly_data/data_for_month_19870630.csv': 3934,\n",
       " 'monthly_data/data_for_month_19870731.csv': 4088,\n",
       " 'monthly_data/data_for_month_19870831.csv': 4080,\n",
       " 'monthly_data/data_for_month_19870930.csv': 4039,\n",
       " 'monthly_data/data_for_month_19871030.csv': 4032,\n",
       " 'monthly_data/data_for_month_19871130.csv': 4004,\n",
       " 'monthly_data/data_for_month_19871231.csv': 3992,\n",
       " 'monthly_data/data_for_month_19880129.csv': 4010,\n",
       " 'monthly_data/data_for_month_19880229.csv': 3987,\n",
       " 'monthly_data/data_for_month_19880331.csv': 3972,\n",
       " 'monthly_data/data_for_month_19880429.csv': 4006,\n",
       " 'monthly_data/data_for_month_19880531.csv': 3996,\n",
       " 'monthly_data/data_for_month_19880630.csv': 3975,\n",
       " 'monthly_data/data_for_month_19880729.csv': 4258,\n",
       " 'monthly_data/data_for_month_19880831.csv': 4236,\n",
       " 'monthly_data/data_for_month_19880930.csv': 4193,\n",
       " 'monthly_data/data_for_month_19881031.csv': 4232,\n",
       " 'monthly_data/data_for_month_19881130.csv': 4209,\n",
       " 'monthly_data/data_for_month_19881230.csv': 4170,\n",
       " 'monthly_data/data_for_month_19890131.csv': 4204,\n",
       " 'monthly_data/data_for_month_19890228.csv': 4169,\n",
       " 'monthly_data/data_for_month_19890331.csv': 4162,\n",
       " 'monthly_data/data_for_month_19890428.csv': 4203,\n",
       " 'monthly_data/data_for_month_19890531.csv': 4195,\n",
       " 'monthly_data/data_for_month_19890630.csv': 4190,\n",
       " 'monthly_data/data_for_month_19890731.csv': 4547,\n",
       " 'monthly_data/data_for_month_19890831.csv': 4528,\n",
       " 'monthly_data/data_for_month_19890929.csv': 4496,\n",
       " 'monthly_data/data_for_month_19891031.csv': 4516,\n",
       " 'monthly_data/data_for_month_19891130.csv': 4483,\n",
       " 'monthly_data/data_for_month_19891229.csv': 4464,\n",
       " 'monthly_data/data_for_month_19900131.csv': 4492,\n",
       " 'monthly_data/data_for_month_19900228.csv': 4464,\n",
       " 'monthly_data/data_for_month_19900330.csv': 4456,\n",
       " 'monthly_data/data_for_month_19900430.csv': 4468,\n",
       " 'monthly_data/data_for_month_19900531.csv': 4451,\n",
       " 'monthly_data/data_for_month_19900629.csv': 4427,\n",
       " 'monthly_data/data_for_month_19900731.csv': 4634,\n",
       " 'monthly_data/data_for_month_19900831.csv': 4611,\n",
       " 'monthly_data/data_for_month_19900928.csv': 4570,\n",
       " 'monthly_data/data_for_month_19901031.csv': 4568,\n",
       " 'monthly_data/data_for_month_19901130.csv': 4519,\n",
       " 'monthly_data/data_for_month_19901231.csv': 4501,\n",
       " 'monthly_data/data_for_month_19910131.csv': 4515,\n",
       " 'monthly_data/data_for_month_19910228.csv': 4484,\n",
       " 'monthly_data/data_for_month_19910328.csv': 4464,\n",
       " 'monthly_data/data_for_month_19910430.csv': 4462,\n",
       " 'monthly_data/data_for_month_19910531.csv': 4441,\n",
       " 'monthly_data/data_for_month_19910628.csv': 4441,\n",
       " 'monthly_data/data_for_month_19910731.csv': 4680,\n",
       " 'monthly_data/data_for_month_19910830.csv': 4635,\n",
       " 'monthly_data/data_for_month_19910930.csv': 4607,\n",
       " 'monthly_data/data_for_month_19911031.csv': 4601,\n",
       " 'monthly_data/data_for_month_19911129.csv': 4571,\n",
       " 'monthly_data/data_for_month_19911231.csv': 4548,\n",
       " 'monthly_data/data_for_month_19920131.csv': 4548,\n",
       " 'monthly_data/data_for_month_19920228.csv': 4516,\n",
       " 'monthly_data/data_for_month_19920331.csv': 4506,\n",
       " 'monthly_data/data_for_month_19920430.csv': 4522,\n",
       " 'monthly_data/data_for_month_19920529.csv': 4495,\n",
       " 'monthly_data/data_for_month_19920630.csv': 4474,\n",
       " 'monthly_data/data_for_month_19920731.csv': 4643,\n",
       " 'monthly_data/data_for_month_19920831.csv': 4608,\n",
       " 'monthly_data/data_for_month_19920930.csv': 4568,\n",
       " 'monthly_data/data_for_month_19921030.csv': 4564,\n",
       " 'monthly_data/data_for_month_19921130.csv': 4521,\n",
       " 'monthly_data/data_for_month_19921231.csv': 4515,\n",
       " 'monthly_data/data_for_month_19930129.csv': 4538,\n",
       " 'monthly_data/data_for_month_19930226.csv': 4511,\n",
       " 'monthly_data/data_for_month_19930331.csv': 4494,\n",
       " 'monthly_data/data_for_month_19930430.csv': 4502,\n",
       " 'monthly_data/data_for_month_19930528.csv': 4491,\n",
       " 'monthly_data/data_for_month_19930630.csv': 4494,\n",
       " 'monthly_data/data_for_month_19930730.csv': 4790,\n",
       " 'monthly_data/data_for_month_19930831.csv': 4781,\n",
       " 'monthly_data/data_for_month_19930930.csv': 4767,\n",
       " 'monthly_data/data_for_month_19931029.csv': 4775,\n",
       " 'monthly_data/data_for_month_19931130.csv': 4751,\n",
       " 'monthly_data/data_for_month_19931231.csv': 4749,\n",
       " 'monthly_data/data_for_month_19940131.csv': 4792,\n",
       " 'monthly_data/data_for_month_19940228.csv': 4760,\n",
       " 'monthly_data/data_for_month_19940331.csv': 4755,\n",
       " 'monthly_data/data_for_month_19940429.csv': 4785,\n",
       " 'monthly_data/data_for_month_19940531.csv': 4776,\n",
       " 'monthly_data/data_for_month_19940630.csv': 4773,\n",
       " 'monthly_data/data_for_month_19940729.csv': 5119,\n",
       " 'monthly_data/data_for_month_19940831.csv': 5111,\n",
       " 'monthly_data/data_for_month_19940930.csv': 5076,\n",
       " 'monthly_data/data_for_month_19941031.csv': 5082,\n",
       " 'monthly_data/data_for_month_19941130.csv': 5051,\n",
       " 'monthly_data/data_for_month_19941230.csv': 5024,\n",
       " 'monthly_data/data_for_month_19950131.csv': 5054,\n",
       " 'monthly_data/data_for_month_19950228.csv': 5024,\n",
       " 'monthly_data/data_for_month_19950331.csv': 5012,\n",
       " 'monthly_data/data_for_month_19950428.csv': 5051,\n",
       " 'monthly_data/data_for_month_19950531.csv': 5044,\n",
       " 'monthly_data/data_for_month_19950630.csv': 5026,\n",
       " 'monthly_data/data_for_month_19950731.csv': 5859,\n",
       " 'monthly_data/data_for_month_19950831.csv': 5856,\n",
       " 'monthly_data/data_for_month_19950929.csv': 5818,\n",
       " 'monthly_data/data_for_month_19951031.csv': 5847,\n",
       " 'monthly_data/data_for_month_19951130.csv': 5801,\n",
       " 'monthly_data/data_for_month_19951229.csv': 5782,\n",
       " 'monthly_data/data_for_month_19960131.csv': 5801,\n",
       " 'monthly_data/data_for_month_19960229.csv': 5759,\n",
       " 'monthly_data/data_for_month_19960329.csv': 5740,\n",
       " 'monthly_data/data_for_month_19960430.csv': 5758,\n",
       " 'monthly_data/data_for_month_19960531.csv': 5748,\n",
       " 'monthly_data/data_for_month_19960628.csv': 5734,\n",
       " 'monthly_data/data_for_month_19960731.csv': 6274,\n",
       " 'monthly_data/data_for_month_19960830.csv': 6239,\n",
       " 'monthly_data/data_for_month_19960930.csv': 6181,\n",
       " 'monthly_data/data_for_month_19961031.csv': 6191,\n",
       " 'monthly_data/data_for_month_19961129.csv': 6142,\n",
       " 'monthly_data/data_for_month_19961231.csv': 6127,\n",
       " 'monthly_data/data_for_month_19970131.csv': 6134,\n",
       " 'monthly_data/data_for_month_19970228.csv': 6072,\n",
       " 'monthly_data/data_for_month_19970331.csv': 6049,\n",
       " 'monthly_data/data_for_month_19970430.csv': 6070,\n",
       " 'monthly_data/data_for_month_19970530.csv': 6044,\n",
       " 'monthly_data/data_for_month_19970630.csv': 6023,\n",
       " 'monthly_data/data_for_month_19970731.csv': 6555,\n",
       " 'monthly_data/data_for_month_19970829.csv': 6502,\n",
       " 'monthly_data/data_for_month_19970930.csv': 6417,\n",
       " 'monthly_data/data_for_month_19971031.csv': 6412,\n",
       " 'monthly_data/data_for_month_19971128.csv': 6354,\n",
       " 'monthly_data/data_for_month_19971231.csv': 6315,\n",
       " 'monthly_data/data_for_month_19980130.csv': 6326,\n",
       " 'monthly_data/data_for_month_19980227.csv': 6256,\n",
       " 'monthly_data/data_for_month_19980331.csv': 6225,\n",
       " 'monthly_data/data_for_month_19980430.csv': 6221,\n",
       " 'monthly_data/data_for_month_19980529.csv': 6180,\n",
       " 'monthly_data/data_for_month_19980630.csv': 6157,\n",
       " 'monthly_data/data_for_month_19980731.csv': 6810,\n",
       " 'monthly_data/data_for_month_19980831.csv': 6748,\n",
       " 'monthly_data/data_for_month_19980930.csv': 6664,\n",
       " 'monthly_data/data_for_month_19981030.csv': 6607,\n",
       " 'monthly_data/data_for_month_19981130.csv': 6479,\n",
       " 'monthly_data/data_for_month_19981231.csv': 6418,\n",
       " 'monthly_data/data_for_month_19990129.csv': 6362,\n",
       " 'monthly_data/data_for_month_19990226.csv': 6270,\n",
       " 'monthly_data/data_for_month_19990331.csv': 6224,\n",
       " 'monthly_data/data_for_month_19990430.csv': 6186,\n",
       " 'monthly_data/data_for_month_19990528.csv': 6134,\n",
       " 'monthly_data/data_for_month_19990630.csv': 6100,\n",
       " 'monthly_data/data_for_month_19990730.csv': 6561,\n",
       " 'monthly_data/data_for_month_19990831.csv': 6490,\n",
       " 'monthly_data/data_for_month_19990930.csv': 6404,\n",
       " 'monthly_data/data_for_month_19991029.csv': 6347,\n",
       " 'monthly_data/data_for_month_19991130.csv': 6256,\n",
       " 'monthly_data/data_for_month_19991231.csv': 6172,\n",
       " 'monthly_data/data_for_month_20000131.csv': 6163,\n",
       " 'monthly_data/data_for_month_20000229.csv': 6102,\n",
       " 'monthly_data/data_for_month_20000331.csv': 6049,\n",
       " 'monthly_data/data_for_month_20000428.csv': 6025,\n",
       " 'monthly_data/data_for_month_20000531.csv': 5982,\n",
       " 'monthly_data/data_for_month_20000630.csv': 5945,\n",
       " 'monthly_data/data_for_month_20000731.csv': 6274,\n",
       " 'monthly_data/data_for_month_20000831.csv': 6232,\n",
       " 'monthly_data/data_for_month_20000929.csv': 6142,\n",
       " 'monthly_data/data_for_month_20001031.csv': 6091,\n",
       " 'monthly_data/data_for_month_20001130.csv': 5982,\n",
       " 'monthly_data/data_for_month_20001229.csv': 5904,\n",
       " 'monthly_data/data_for_month_20010131.csv': 5857,\n",
       " 'monthly_data/data_for_month_20010228.csv': 5774,\n",
       " 'monthly_data/data_for_month_20010330.csv': 5715,\n",
       " 'monthly_data/data_for_month_20010430.csv': 5691,\n",
       " 'monthly_data/data_for_month_20010531.csv': 5630,\n",
       " 'monthly_data/data_for_month_20010629.csv': 5583,\n",
       " 'monthly_data/data_for_month_20010731.csv': 6096,\n",
       " 'monthly_data/data_for_month_20010831.csv': 6029,\n",
       " 'monthly_data/data_for_month_20010928.csv': 5931,\n",
       " 'monthly_data/data_for_month_20011031.csv': 5909,\n",
       " 'monthly_data/data_for_month_20011130.csv': 5849,\n",
       " 'monthly_data/data_for_month_20011231.csv': 5792,\n",
       " 'monthly_data/data_for_month_20020131.csv': 5778,\n",
       " 'monthly_data/data_for_month_20020228.csv': 5733,\n",
       " 'monthly_data/data_for_month_20020328.csv': 5693,\n",
       " 'monthly_data/data_for_month_20020430.csv': 5669,\n",
       " 'monthly_data/data_for_month_20020531.csv': 5617,\n",
       " 'monthly_data/data_for_month_20020628.csv': 5572,\n",
       " 'monthly_data/data_for_month_20020731.csv': 6016,\n",
       " 'monthly_data/data_for_month_20020830.csv': 5956,\n",
       " 'monthly_data/data_for_month_20020930.csv': 5893,\n",
       " 'monthly_data/data_for_month_20021031.csv': 5870,\n",
       " 'monthly_data/data_for_month_20021129.csv': 5780,\n",
       " 'monthly_data/data_for_month_20021231.csv': 5742,\n",
       " 'monthly_data/data_for_month_20030131.csv': 5704,\n",
       " 'monthly_data/data_for_month_20030228.csv': 5632,\n",
       " 'monthly_data/data_for_month_20030331.csv': 5580,\n",
       " 'monthly_data/data_for_month_20030430.csv': 5539,\n",
       " 'monthly_data/data_for_month_20030530.csv': 5488,\n",
       " 'monthly_data/data_for_month_20030630.csv': 5461,\n",
       " 'monthly_data/data_for_month_20030731.csv': 5610,\n",
       " 'monthly_data/data_for_month_20030829.csv': 5559,\n",
       " 'monthly_data/data_for_month_20030930.csv': 5503,\n",
       " 'monthly_data/data_for_month_20031031.csv': 5486,\n",
       " 'monthly_data/data_for_month_20031128.csv': 5441,\n",
       " 'monthly_data/data_for_month_20031231.csv': 5405,\n",
       " 'monthly_data/data_for_month_20040130.csv': 5375,\n",
       " 'monthly_data/data_for_month_20040227.csv': 5322,\n",
       " 'monthly_data/data_for_month_20040331.csv': 5299,\n",
       " 'monthly_data/data_for_month_20040430.csv': 5291,\n",
       " 'monthly_data/data_for_month_20040528.csv': 5260,\n",
       " 'monthly_data/data_for_month_20040630.csv': 5234,\n",
       " 'monthly_data/data_for_month_20040730.csv': 5376,\n",
       " 'monthly_data/data_for_month_20040831.csv': 5342,\n",
       " 'monthly_data/data_for_month_20040930.csv': 5301,\n",
       " 'monthly_data/data_for_month_20041029.csv': 5301,\n",
       " 'monthly_data/data_for_month_20041130.csv': 5261,\n",
       " 'monthly_data/data_for_month_20041231.csv': 5235,\n",
       " 'monthly_data/data_for_month_20050131.csv': 5202,\n",
       " 'monthly_data/data_for_month_20050228.csv': 5169,\n",
       " 'monthly_data/data_for_month_20050331.csv': 5150,\n",
       " 'monthly_data/data_for_month_20050429.csv': 5133,\n",
       " 'monthly_data/data_for_month_20050531.csv': 5103,\n",
       " 'monthly_data/data_for_month_20050630.csv': 5075,\n",
       " 'monthly_data/data_for_month_20050729.csv': 5251,\n",
       " 'monthly_data/data_for_month_20050831.csv': 5210,\n",
       " 'monthly_data/data_for_month_20050930.csv': 5157,\n",
       " 'monthly_data/data_for_month_20051031.csv': 5130,\n",
       " 'monthly_data/data_for_month_20051130.csv': 5089,\n",
       " 'monthly_data/data_for_month_20051230.csv': 5058,\n",
       " 'monthly_data/data_for_month_20060131.csv': 5022,\n",
       " 'monthly_data/data_for_month_20060228.csv': 4980,\n",
       " 'monthly_data/data_for_month_20060331.csv': 4956,\n",
       " 'monthly_data/data_for_month_20060428.csv': 4935,\n",
       " 'monthly_data/data_for_month_20060531.csv': 4912,\n",
       " 'monthly_data/data_for_month_20060630.csv': 4890,\n",
       " 'monthly_data/data_for_month_20060731.csv': 5177,\n",
       " 'monthly_data/data_for_month_20060831.csv': 5145,\n",
       " 'monthly_data/data_for_month_20060929.csv': 5112,\n",
       " 'monthly_data/data_for_month_20061031.csv': 5097,\n",
       " 'monthly_data/data_for_month_20061130.csv': 5038,\n",
       " 'monthly_data/data_for_month_20061229.csv': 4999,\n",
       " 'monthly_data/data_for_month_20070131.csv': 4968,\n",
       " 'monthly_data/data_for_month_20070228.csv': 4910,\n",
       " 'monthly_data/data_for_month_20070330.csv': 4882,\n",
       " 'monthly_data/data_for_month_20070430.csv': 4865,\n",
       " 'monthly_data/data_for_month_20070531.csv': 4824,\n",
       " 'monthly_data/data_for_month_20070629.csv': 4801,\n",
       " 'monthly_data/data_for_month_20070731.csv': 5063,\n",
       " 'monthly_data/data_for_month_20070831.csv': 5018,\n",
       " 'monthly_data/data_for_month_20070928.csv': 4959,\n",
       " 'monthly_data/data_for_month_20071031.csv': 4917,\n",
       " 'monthly_data/data_for_month_20071130.csv': 4863,\n",
       " 'monthly_data/data_for_month_20071231.csv': 4809,\n",
       " 'monthly_data/data_for_month_20080131.csv': 4783,\n",
       " 'monthly_data/data_for_month_20080229.csv': 4751,\n",
       " 'monthly_data/data_for_month_20080331.csv': 4719,\n",
       " 'monthly_data/data_for_month_20080430.csv': 4714,\n",
       " 'monthly_data/data_for_month_20080530.csv': 4683,\n",
       " 'monthly_data/data_for_month_20080630.csv': 4656,\n",
       " 'monthly_data/data_for_month_20080731.csv': 4924,\n",
       " 'monthly_data/data_for_month_20080829.csv': 4886,\n",
       " 'monthly_data/data_for_month_20080930.csv': 4846,\n",
       " 'monthly_data/data_for_month_20081031.csv': 4824,\n",
       " 'monthly_data/data_for_month_20081128.csv': 4771,\n",
       " 'monthly_data/data_for_month_20081231.csv': 4743,\n",
       " 'monthly_data/data_for_month_20090130.csv': 4712,\n",
       " 'monthly_data/data_for_month_20090227.csv': 4675,\n",
       " 'monthly_data/data_for_month_20090331.csv': 4649,\n",
       " 'monthly_data/data_for_month_20090430.csv': 4627,\n",
       " 'monthly_data/data_for_month_20090529.csv': 4593,\n",
       " 'monthly_data/data_for_month_20090630.csv': 4576,\n",
       " 'monthly_data/data_for_month_20090731.csv': 4837,\n",
       " 'monthly_data/data_for_month_20090831.csv': 4800,\n",
       " 'monthly_data/data_for_month_20090930.csv': 4776,\n",
       " 'monthly_data/data_for_month_20091030.csv': 4766,\n",
       " 'monthly_data/data_for_month_20091130.csv': 4724,\n",
       " 'monthly_data/data_for_month_20091231.csv': 4691,\n",
       " 'monthly_data/data_for_month_20100129.csv': 4661,\n",
       " 'monthly_data/data_for_month_20100226.csv': 4632,\n",
       " 'monthly_data/data_for_month_20100331.csv': 4620,\n",
       " 'monthly_data/data_for_month_20100430.csv': 4595,\n",
       " 'monthly_data/data_for_month_20100528.csv': 4573,\n",
       " 'monthly_data/data_for_month_20100630.csv': 4560,\n",
       " 'monthly_data/data_for_month_20100730.csv': 4659,\n",
       " 'monthly_data/data_for_month_20100831.csv': 4612,\n",
       " 'monthly_data/data_for_month_20100930.csv': 4576,\n",
       " 'monthly_data/data_for_month_20101029.csv': 4551,\n",
       " 'monthly_data/data_for_month_20101130.csv': 4519,\n",
       " 'monthly_data/data_for_month_20101231.csv': 4484,\n",
       " 'monthly_data/data_for_month_20110131.csv': 4462,\n",
       " 'monthly_data/data_for_month_20110228.csv': 4441,\n",
       " 'monthly_data/data_for_month_20110331.csv': 4422,\n",
       " 'monthly_data/data_for_month_20110429.csv': 4404,\n",
       " 'monthly_data/data_for_month_20110531.csv': 4374,\n",
       " 'monthly_data/data_for_month_20110630.csv': 4352,\n",
       " 'monthly_data/data_for_month_20110729.csv': 4452,\n",
       " 'monthly_data/data_for_month_20110831.csv': 4432,\n",
       " 'monthly_data/data_for_month_20110930.csv': 4409,\n",
       " 'monthly_data/data_for_month_20111031.csv': 4396,\n",
       " 'monthly_data/data_for_month_20111130.csv': 4362,\n",
       " 'monthly_data/data_for_month_20111230.csv': 4330,\n",
       " 'monthly_data/data_for_month_20120131.csv': 4323,\n",
       " 'monthly_data/data_for_month_20120229.csv': 4294,\n",
       " 'monthly_data/data_for_month_20120330.csv': 4276,\n",
       " 'monthly_data/data_for_month_20120430.csv': 4262,\n",
       " 'monthly_data/data_for_month_20120531.csv': 4240,\n",
       " 'monthly_data/data_for_month_20120629.csv': 4226,\n",
       " 'monthly_data/data_for_month_20120731.csv': 4414,\n",
       " 'monthly_data/data_for_month_20120831.csv': 4392,\n",
       " 'monthly_data/data_for_month_20120928.csv': 4373,\n",
       " 'monthly_data/data_for_month_20121031.csv': 4354,\n",
       " 'monthly_data/data_for_month_20121130.csv': 4315,\n",
       " 'monthly_data/data_for_month_20121231.csv': 4294,\n",
       " 'monthly_data/data_for_month_20130131.csv': 4275,\n",
       " 'monthly_data/data_for_month_20130228.csv': 4250,\n",
       " 'monthly_data/data_for_month_20130328.csv': 4232,\n",
       " 'monthly_data/data_for_month_20130430.csv': 4222,\n",
       " 'monthly_data/data_for_month_20130531.csv': 4193,\n",
       " 'monthly_data/data_for_month_20130628.csv': 4171,\n",
       " 'monthly_data/data_for_month_20130731.csv': 4325,\n",
       " 'monthly_data/data_for_month_20130830.csv': 4300,\n",
       " 'monthly_data/data_for_month_20130930.csv': 4279,\n",
       " 'monthly_data/data_for_month_20131031.csv': 4268,\n",
       " 'monthly_data/data_for_month_20131129.csv': 4236,\n",
       " 'monthly_data/data_for_month_20131231.csv': 4213,\n",
       " 'monthly_data/data_for_month_20140131.csv': 4198,\n",
       " 'monthly_data/data_for_month_20140228.csv': 4180,\n",
       " 'monthly_data/data_for_month_20140331.csv': 4157,\n",
       " 'monthly_data/data_for_month_20140430.csv': 4151,\n",
       " 'monthly_data/data_for_month_20140530.csv': 4129,\n",
       " 'monthly_data/data_for_month_20140630.csv': 4119,\n",
       " 'monthly_data/data_for_month_20140731.csv': 4298,\n",
       " 'monthly_data/data_for_month_20140829.csv': 4281,\n",
       " 'monthly_data/data_for_month_20140930.csv': 4263,\n",
       " 'monthly_data/data_for_month_20141031.csv': 4252,\n",
       " 'monthly_data/data_for_month_20141128.csv': 4225,\n",
       " 'monthly_data/data_for_month_20141231.csv': 4211,\n",
       " 'monthly_data/data_for_month_20150130.csv': 4195,\n",
       " 'monthly_data/data_for_month_20150227.csv': 4168,\n",
       " 'monthly_data/data_for_month_20150331.csv': 4143,\n",
       " 'monthly_data/data_for_month_20150430.csv': 4131,\n",
       " 'monthly_data/data_for_month_20150529.csv': 4117,\n",
       " 'monthly_data/data_for_month_20150630.csv': 4109,\n",
       " 'monthly_data/data_for_month_20150731.csv': 4372,\n",
       " 'monthly_data/data_for_month_20150831.csv': 4338,\n",
       " 'monthly_data/data_for_month_20150930.csv': 4314,\n",
       " 'monthly_data/data_for_month_20151030.csv': 4297,\n",
       " 'monthly_data/data_for_month_20151130.csv': 4258,\n",
       " 'monthly_data/data_for_month_20151231.csv': 4243,\n",
       " 'monthly_data/data_for_month_20160129.csv': 4215,\n",
       " 'monthly_data/data_for_month_20160229.csv': 4179,\n",
       " 'monthly_data/data_for_month_20160331.csv': 4157,\n",
       " 'monthly_data/data_for_month_20160429.csv': 4145,\n",
       " 'monthly_data/data_for_month_20160531.csv': 4108,\n",
       " 'monthly_data/data_for_month_20160630.csv': 4084,\n",
       " 'monthly_data/data_for_month_20160729.csv': 4416,\n",
       " 'monthly_data/data_for_month_20160831.csv': 4388,\n",
       " 'monthly_data/data_for_month_20160930.csv': 4354,\n",
       " 'monthly_data/data_for_month_20161031.csv': 4334,\n",
       " 'monthly_data/data_for_month_20161130.csv': 4307,\n",
       " 'monthly_data/data_for_month_20161230.csv': 4279,\n",
       " 'monthly_data/data_for_month_20170131.csv': 4271,\n",
       " 'monthly_data/data_for_month_20170228.csv': 4246,\n",
       " 'monthly_data/data_for_month_20170331.csv': 4224,\n",
       " 'monthly_data/data_for_month_20170428.csv': 4210,\n",
       " 'monthly_data/data_for_month_20170531.csv': 4189,\n",
       " 'monthly_data/data_for_month_20170630.csv': 4175,\n",
       " 'monthly_data/data_for_month_20170731.csv': 4359,\n",
       " 'monthly_data/data_for_month_20170831.csv': 4337,\n",
       " 'monthly_data/data_for_month_20170929.csv': 4303,\n",
       " 'monthly_data/data_for_month_20171031.csv': 4279,\n",
       " 'monthly_data/data_for_month_20171130.csv': 4251,\n",
       " 'monthly_data/data_for_month_20171229.csv': 4220,\n",
       " 'monthly_data/data_for_month_20180131.csv': 4197,\n",
       " 'monthly_data/data_for_month_20180228.csv': 4186,\n",
       " 'monthly_data/data_for_month_20180329.csv': 4172,\n",
       " 'monthly_data/data_for_month_20180430.csv': 4158,\n",
       " 'monthly_data/data_for_month_20180531.csv': 4134,\n",
       " 'monthly_data/data_for_month_20180629.csv': 4122,\n",
       " 'monthly_data/data_for_month_20180731.csv': 4295,\n",
       " 'monthly_data/data_for_month_20180831.csv': 4256,\n",
       " 'monthly_data/data_for_month_20180928.csv': 4237,\n",
       " 'monthly_data/data_for_month_20181031.csv': 4226,\n",
       " 'monthly_data/data_for_month_20181130.csv': 4200,\n",
       " 'monthly_data/data_for_month_20181231.csv': 4180,\n",
       " 'monthly_data/data_for_month_20190131.csv': 4153,\n",
       " 'monthly_data/data_for_month_20190228.csv': 4127,\n",
       " 'monthly_data/data_for_month_20190329.csv': 4109,\n",
       " 'monthly_data/data_for_month_20190430.csv': 4088,\n",
       " 'monthly_data/data_for_month_20190531.csv': 4075,\n",
       " 'monthly_data/data_for_month_20190628.csv': 4058,\n",
       " 'monthly_data/data_for_month_20190731.csv': 4238,\n",
       " 'monthly_data/data_for_month_20190830.csv': 4186,\n",
       " 'monthly_data/data_for_month_20190930.csv': 4164,\n",
       " 'monthly_data/data_for_month_20191031.csv': 4144,\n",
       " 'monthly_data/data_for_month_20191129.csv': 4114,\n",
       " 'monthly_data/data_for_month_20191231.csv': 4090,\n",
       " 'monthly_data/data_for_month_20200131.csv': 4065,\n",
       " 'monthly_data/data_for_month_20200228.csv': 4045,\n",
       " 'monthly_data/data_for_month_20200331.csv': 4033,\n",
       " 'monthly_data/data_for_month_20200430.csv': 4021,\n",
       " 'monthly_data/data_for_month_20200529.csv': 3996,\n",
       " 'monthly_data/data_for_month_20200630.csv': 3975,\n",
       " 'monthly_data/data_for_month_20200731.csv': 3974,\n",
       " 'monthly_data/data_for_month_20200831.csv': 3929,\n",
       " 'monthly_data/data_for_month_20200930.csv': 3913,\n",
       " 'monthly_data/data_for_month_20201030.csv': 3900,\n",
       " 'monthly_data/data_for_month_20201130.csv': 3878,\n",
       " 'monthly_data/data_for_month_20201231.csv': 3862}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138634, 70)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519707"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(number_of_obs_list.values())[:421])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749032"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(number_of_obs_list.values())[:481])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('characters_pca.pkl', 'rb') as f:\n",
    "    Z = pickle.load(f)\n",
    "with open('factors_pca.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('returns_pca.pkl', 'rb') as f:\n",
    "    returns = pickle.load(f)\n",
    "with open('number_of_obs_pca.pkl', 'rb') as f:\n",
    "    number_of_obs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = Z[749032:]\n",
    "x_test = X[749032:]\n",
    "returns_test = returns[749032:]\n",
    "z_x_test = np.hstack((z_test, x_test))\n",
    "\n",
    "z_x_train = np.concatenate((Z[:519707], X[:519707]), axis=1)\n",
    "z_x_val = np.concatenate((Z[519707:749032], X[519707:749032]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_3/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_3'\")\n",
      "Epoch 1/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0228 - val_loss: 0.0180\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0210 - val_loss: 0.0179\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0209 - val_loss: 0.0179\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0208 - val_loss: 0.0179\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0208 - val_loss: 0.0179\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0207 - val_loss: 0.0179\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0207 - val_loss: 0.0179\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0207 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0207 - val_loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=3, activ='relu', l_rate=0.002,\n",
    "                                     inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                    kernel_init='he_uniform', optimizer=keras.optimizers.Adam)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 6s 526us/step\n",
      "23408/23408 [==============================] - 13s 536us/step\n",
      "Out of Sample R^2: 0.13307985210577045\n",
      "Total R^2: 0.17650151063871866\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted R^2: -0.004658250038879874\n"
     ]
    }
   ],
   "source": [
    "predicted_r_2 = create_total_r2(returns_test, predicted)\n",
    "print(f\"Predicted R^2: {predicted_r_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0221 - val_loss: 0.0182\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0211 - val_loss: 0.0181\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0209 - val_loss: 0.0180\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0207 - val_loss: 0.0180\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0206 - val_loss: 0.0181\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0205 - val_loss: 0.0180\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0205 - val_loss: 0.0181\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0204 - val_loss: 0.0181\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0203 - val_loss: 0.0181\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=3, activ='relu', l_rate=0.002,\n",
    "                                           inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                           kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 7s 559us/step\n",
      "23408/23408 [==============================] - 14s 577us/step\n",
      "Out of Sample R^2: 0.12237729958348809\n",
      "Total R^2: 0.18598202865237745\n",
      " Predictive R^2: 0.0018282305884290695\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\" Predictive R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.0221 - val_loss: 0.0181\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0212 - val_loss: 0.0182\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0210 - val_loss: 0.0183\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0208 - val_loss: 0.0181\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0206 - val_loss: 0.0180\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0204 - val_loss: 0.0181\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0204 - val_loss: 0.0180\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0203 - val_loss: 0.0182\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0202 - val_loss: 0.0180\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0201 - val_loss: 0.0180\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0199 - val_loss: 0.0181\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0198 - val_loss: 0.0181\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0,\n",
    "                                            reg_type=keras.regularizers.L2, kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test,returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 7s 602us/step\n",
      "23408/23408 [==============================] - 13s 561us/step\n",
      "Out of Sample R^2: 0.1254760225509136\n",
      "Total R^2: 0.2049515182645817\n",
      "Predictiv R^2: 0.004107272395861505\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\"Predictiv R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_9/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_9'\")\n",
      "Epoch 1/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0221 - val_loss: 0.0179\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0179\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0178\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0179\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0205 - val_loss: 0.0179\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0204 - val_loss: 0.0178\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0204 - val_loss: 0.0179\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0204 - val_loss: 0.0178\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0204 - val_loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=5, activ='relu', l_rate=0.002,\n",
    "                                     inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                    kernel_init='he_uniform', optimizer=keras.optimizers.Adam)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 6s 529us/step\n",
      "23408/23408 [==============================] - 12s 528us/step\n",
      "Out of Sample R^2: 0.13354397392278805\n",
      "Total R^2: 0.18259116005488152\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted R^2: -0.012878600061789758\n"
     ]
    }
   ],
   "source": [
    "predicted_r_2 = create_total_r2(returns_test, predicted)\n",
    "print(f\"Predicted R^2: {predicted_r_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0219 - val_loss: 0.0179\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0208 - val_loss: 0.0178\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0205 - val_loss: 0.0180\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0204 - val_loss: 0.0178\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0203 - val_loss: 0.0179\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0202 - val_loss: 0.0178\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0201 - val_loss: 0.0181\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0200 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0200 - val_loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=5, activ='relu', l_rate=0.002,\n",
    "                                           inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                           kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 7s 536us/step\n",
      "23408/23408 [==============================] - 13s 536us/step\n",
      "Out of Sample R^2: 0.1341085208784515\n",
      "Total R^2: 0.19925341219324955\n",
      " Predictive R^2: 0.004893425112243777\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\" Predictive R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.0217 - val_loss: 0.0180\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0207 - val_loss: 0.0179\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0205 - val_loss: 0.0179\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0204 - val_loss: 0.0179\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0202 - val_loss: 0.0181\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.0201 - val_loss: 0.0180\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=5, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0,\n",
    "                                            reg_type=keras.regularizers.L2, kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[:749032],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test,returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 7s 533us/step\n",
      "23408/23408 [==============================] - 12s 526us/step\n",
      "Out of Sample R^2: 0.12994331245888646\n",
      "Total R^2: 0.19789018770990574\n",
      "Predictiv R^2: 0.002127911838298191\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\"Predictiv R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation only Last 10 Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341690"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(number_of_obs_list.values())[:362])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516341"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(number_of_obs_list.values())[:421])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744997"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(number_of_obs_list.values())[:481])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = Z[744997:]\n",
    "x_test = X[744997:]\n",
    "returns_test = returns[744997:]\n",
    "z_x_test = np.hstack((z_test, x_test))\n",
    "\n",
    "returns_eval = returns[341690:]\n",
    "z_x_train = np.concatenate((Z[341690:516341], X[341690:516341]), axis=1)\n",
    "z_x_val = np.concatenate((Z[516341:744997], X[516341:744997]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_17/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_17'\")\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0256 - val_loss: 0.0184\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0242 - val_loss: 0.0185\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0239 - val_loss: 0.0183\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0237 - val_loss: 0.0183\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0235 - val_loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=3, activ='relu', l_rate=0.002,\n",
    "                                     inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                    kernel_init='he_uniform', optimizer=keras.optimizers.Adam)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 561us/step\n",
      "12604/12604 [==============================] - 7s 551us/step\n",
      "Out of Sample R^2: 0.1110539980682621\n",
      "Total R^2: 0.17789112492975412\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted R^2: -0.011517637241458267\n"
     ]
    }
   ],
   "source": [
    "predicted_r_2 = create_total_r2(returns_test, predicted)\n",
    "print(f\"Predicted R^2: {predicted_r_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 2s 4ms/step - loss: 0.0255 - val_loss: 0.0184\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 2s 4ms/step - loss: 0.0239 - val_loss: 0.0184\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 1s 4ms/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 1s 4ms/step - loss: 0.0234 - val_loss: 0.0184\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 1s 4ms/step - loss: 0.0232 - val_loss: 0.0185\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 1s 4ms/step - loss: 0.0231 - val_loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=3, activ='relu', l_rate=0.002,\n",
    "                                           inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                           kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 567us/step\n",
      "12604/12604 [==============================] - 7s 579us/step\n",
      "Out of Sample R^2: 0.10124720141521759\n",
      "Total R^2: 0.18580826891247915\n",
      " Predictive R^2: 0.0028726275103817533\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\" Predictive R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 3s 7ms/step - loss: 0.0254 - val_loss: 0.0185\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 3s 7ms/step - loss: 0.0239 - val_loss: 0.0186\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 3s 6ms/step - loss: 0.0236 - val_loss: 0.0187\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 3s 7ms/step - loss: 0.0233 - val_loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=3, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0,\n",
    "                                            reg_type=keras.regularizers.L2, kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test,returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 600us/step\n",
      "12604/12604 [==============================] - 7s 587us/step\n",
      "Out of Sample R^2: 0.10244002262917884\n",
      "Total R^2: 0.18424791093376625\n",
      "Predictiv R^2: 0.005821496871401766\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\"Predictiv R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.float32, name=None), name='tf.__operators__.getitem_23/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_23'\")\n",
      "Epoch 1/50\n",
      "404/404 [==============================] - 2s 3ms/step - loss: 0.0258 - val_loss: 0.0184\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0239 - val_loss: 0.0184\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0235 - val_loss: 0.0183\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0234 - val_loss: 0.0183\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0233 - val_loss: 0.0184\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0232 - val_loss: 0.0184\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 0.0232 - val_loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "model = create_one_hidden_layer_mod(no_units_activ=16, no_factors=5, activ='relu', l_rate=0.002,\n",
    "                                     inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                    kernel_init='he_uniform', optimizer=keras.optimizers.Adam)\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 555us/step\n",
      "12604/12604 [==============================] - 7s 558us/step\n",
      "Out of Sample R^2: 0.11246264162397945\n",
      "Total R^2: 0.18573563030927287\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model.predict(z_x_test)\n",
    "in_samp = model.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[5].output])\n",
    "lambda_new = get_5th_layer_output([z_x_train])[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "# Get the output of the 5th layer\n",
    "get_4th_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "betas = get_4th_layer_output(z_x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted R^2: -0.008113221259306691\n"
     ]
    }
   ],
   "source": [
    "predicted_r_2 = create_total_r2(returns_test, predicted)\n",
    "print(f\"Predicted R^2: {predicted_r_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 2s 3ms/step - loss: 0.0252 - val_loss: 0.0185\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0237 - val_loss: 0.0185\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0234 - val_loss: 0.0183\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0231 - val_loss: 0.0184\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0229 - val_loss: 0.0187\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0228 - val_loss: 0.0185\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_two_hidden_layer_network(no_units_first=64, no_units_second=32, no_factors=5, activ='relu', l_rate=0.002,\n",
    "                                           inp_shape=z_x_train.shape[1], reg=0, reg_type=keras.regularizers.L2,\n",
    "                                           kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_2.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test, returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 557us/step\n",
      "12604/12604 [==============================] - 7s 557us/step\n",
      "Out of Sample R^2: 0.1056840800723744\n",
      "Total R^2: 0.1931405549807439\n",
      " Predictive R^2: 0.0021178369735930236\n"
     ]
    }
   ],
   "source": [
    "#Use the model weights to predict returns on the validation set\n",
    "pred = model_2.predict(z_x_test)\n",
    "in_samp = model_2.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[6].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_2(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "get_5th_layer_output_2 = K.function([model_2.layers[0].input],\n",
    "                                  [model_2.layers[5].output])\n",
    "betas = get_5th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\" Predictive R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 3s 5ms/step - loss: 0.0250 - val_loss: 0.0184\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 2s 5ms/step - loss: 0.0237 - val_loss: 0.0186\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 2s 4ms/step - loss: 0.0233 - val_loss: 0.0186\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 2s 4ms/step - loss: 0.0230 - val_loss: 0.0185\n"
     ]
    }
   ],
   "source": [
    "model_3 = create_three_hidden_layer_network(no_units_first=64, no_units_second=128, no_units_third=32, no_factors=5, activ='relu', l_rate=0.002, inp_shape=z_x_train.shape[1], reg=0,\n",
    "                                            reg_type=keras.regularizers.L2, kernel_init='he_uniform')\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 50\n",
    "history = model_3.fit(\n",
    "    x=np.concatenate([z_x_train, z_x_val]),\n",
    "    y=returns[341690:744997],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose='auto',\n",
    "    validation_data=(z_x_test,returns_test),\n",
    "    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302/12302 [==============================] - 7s 587us/step\n",
      "12604/12604 [==============================] - 7s 581us/step\n",
      "Out of Sample R^2: 0.10698218439072571\n",
      "Total R^2: 0.19127113167868892\n",
      "Predictiv R^2: -6.334219229064963e-05\n"
     ]
    }
   ],
   "source": [
    "pred = model_3.predict(z_x_test)\n",
    "in_samp = model_3.predict(np.concatenate([z_x_train, z_x_val]))\n",
    "\n",
    "\n",
    "#Calculate the R-squared of the model\n",
    "print(f\"Out of Sample R^2: {create_total_r2(returns_test, np.concatenate(pred))}\")\n",
    "print(f\"Total R^2: {create_total_r2(returns_eval, np.concatenate([np.concatenate(in_samp), np.concatenate(pred)]))}\")\n",
    "\n",
    "get_6th_layer_output_3 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[7].output])\n",
    "\n",
    "lambda_new = get_6th_layer_output_3(z_x_test)[0]\n",
    "lambda_facts = np.mean(lambda_new, axis=0)\n",
    "\n",
    "get_7th_layer_output_2 = K.function([model_3.layers[0].input],\n",
    "                                  [model_3.layers[6].output])\n",
    "betas = get_7th_layer_output_2(z_x_test)[0]\n",
    "predicted = betas * np.tile(lambda_facts,(z_x_test[:,70:].shape[0],1))\n",
    "predicted = np.sum(predicted,axis=1)\n",
    "print(f\"Predictiv R^2: {create_total_r2(returns_test, predicted)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
